\documentclass[acmsmall,authordraft]{acmart}

%%
%% Formatting directives for lhs2TeX
%include polycode.fmt
%format forall(x)   = forall_ x "\hsforall"
%format .           = "\hsdot{" `comp_` "}{" period_ "}"
%format `comp_`     = "\circ "
%format period_     = "\,.\,"
%format forall_     = "\forall "
%format public export = "\ "
\makeatletter

\let\HaskellResetHook\empty
\newcommand*{\AtHaskellReset}[1]{%
    \g@@addto@@macro\HaskellResetHook{#1}}
\newcommand*{\HaskellReset}{\HaskellResetHook}

\global\let\hsforallread\empty

\newcommand\hsforall{\global\let\hsdot=\hsperiodonce}
\newcommand*\hsperiodonce[2]{#2\global\let\hsdot=\hscompose}
\newcommand*\hscompose[2]{#1}

\AtHaskellReset{\global\let\hsdot=\hscompose}

\HaskellReset

\makeatother
\EndFmtInput

\iffalse
\begin{code}
import Data.Vect
import Data.List
\end{code}
\fi

\usepackage{fixme}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{ebproof}
\usepackage{cleveref}
\usepackage{quiver}
\usepackage{xcolor}
\usepackage{lmodern}
\usepackage{anyfontsize}
\usepackage{alltt}

\definecolor{concol}{rgb}{.6,.2,.2}
\renewcommand{\Conid}[1]{{\color{concol}\mathrm{#1}}}

\DeclareMathOperator{\delay}{delay}
\usetikzlibrary{arrows.meta}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.
\fxwarning{Rights}
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{????}
\acmDOI{????}


%%
%% These commands are for a JOURNAL article.
\fxwarning{Journal info}
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%%\acmSubmissionID{123-A56-BU3}

\begin{document}

\title{Ruby}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Omar Tahir}
\email{omar.tahir17@@ic.ac.uk}
\orcid{???}
\affiliation{%
  \institution{Imperial College London}
  \city{London}
  \country{UK}
}

\renewcommand{\shortauthors}{Tahir}

\begin{abstract}
Hardware design is done almost exclusively in two dominant hardware description languages, Verilog and VHDL. Both are declarative languages that work at the signal level, wiring together inputs and outputs of hardware blocks. However, there are many situations when a different sort of programming paradigm is needed. Now that more development is being done in functional programming and high-level synthesis for hardware design, we revisit \emph{Ruby}, a relational HDL created primarily for DSP-like applications.

This paper presents several embeddings of Ruby as a domain-specific language in \emph{Idris 2}, an up-and-coming dependently-typed functional language deeply inspired by Haskell. Algebraic effects are used not only to represent Ruby's syntax, but to assist in writing a type inference algorithm that interprets from one DSL to the next. Effect handlers give semantics to the language; non-standard semantics are given including compiling to Verilog and doing longest path estimation.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
    <concept_id>10011007.10011006.10011050.10011017</concept_id>
    <concept_desc>Software and its engineering~Domain specific languages</concept_desc>
    <concept_significance>500</concept_significance>
</concept>
<concept>
    <concept_id>10010583.10010600.10010628</concept_id>
    <concept_desc>Hardware~Reconfigurable logic and FPGAs</concept_desc>
    <concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Domain specific languages}
\ccsdesc[500]{Hardware~Reconfigurable logic and FPGAs}

\keywords{relational programming, functional programming, hardware design}

\maketitle

\section{Introduction}

Hardware description languages such as Verilog and VHDL are focused on the minutiae of hardware design - which logic is clocked and which is unclocked, which signal connects to where, and so on. By contrast, the mathematical descriptions of the functions these designs are supposed to perform are completely devoid of this information. There is a serious gap between the abstract mathematical operations and the way they are implemented at the hardware level. High-level ideas such as ``a triangular wave of registers'' or ``n-fold composition'' are common in DSP applications, but there is no support for expressing such things in common HDLs.

High-level domain-specific languages can help bridge this gap. They can be used to express the functional properties of a design and do design exploration without worrying about bit-level specifics until later. They also enable high-level, source-to-source program transformations - the earlier the analysis, the more optimisations can be applied. However, there is also the potential to reason about low-level properties in a high-level DSL, and direct the way compilation will be done far down the line - lifting such properties through abstraction levels can be extremely useful \cite{SvenssonDSLExploration14,SheeranHardwareFP05}.

\emph{Ruby} \cite{SheeranDescribingCircuits90} is one such language - being relational, it is able to represent a far larger class of programs than a functional language. It is also extremely compact and simple, making it perfect as an embedded DSL. Much exploration has been done on Ruby, both on the theoretical side in type theory and program derivation \cite{HuttonThesis92} and on practical implementation and non-standard analysis \cite{SinghHWAnalysis91}. However, current implementations of Ruby are done in Lazy ML, and provide only relational semantics, despite there being a notion of functional semantics under certain conditions \cite{HuttonRelations99}.

Since the invention of Ruby, \emph{Algebraic effects} have developed as a new way of building domain-specific languages. They enable the rapid construction and composition of DSLs, both shallowly and deeply embedded. While modular algebraic effects have the same expressive power as algebraic monads \cite{SchrijversMonad19}, they supercede them in ease-of-use, namely the ease of composition \cite{SwierstraDTC08} and of writing new interpreters/semantics.

In this paper, we will use algebraic effects as our foundation for implementing Ruby as a DSL. Our host language will be \emph{Idris 2}, a dependently-typed language based on quantative type theory \cite{BradyIdrisQTT21} and strongly inspired by Haskell. The paper is structued as follows:

\begin{itemize}
    \item In \cref{sec:ruby} we introduce Ruby, its syntax and rules.
    \item In \cref{sec:effects} we introduce algebraic effects, motivate their usage, and develop a basic framework for defining and working with them within Idris.
    \item In \cref{sec:dsl} we develop a sequence of DSLs for Ruby. We develop simple typing rules for the language, where each DSL has stronger rules than the last.
    \item In \cref{sec:causal} we describe Hutton's idea of \emph{causal relations}, and use them to give Ruby programs functional semantics and compile them into first-class Idris functions for simulation.
    \item In \cref{sec:nsi} we leverage the power and flexibility of algebraic effects to provide multiple non-standard interpretations of Ruby programs.
\end{itemize}


\section{Ruby}
\label{sec:ruby}

\emph{Ruby} is a simple relational language used for hardware descriptions.

A relation on sets $A$ and $B$ is a subset $R \subseteq A \times B$, written $R : A \leftrightarrow B$. If $(a,b) \in R$ then we write $a R b$, which should be read as ``$a$ is related to $b$ by $R$''. Most properties and relationships between data can be represented by relations. In particular, every function is a relation: if $f : A \to B$ then the relation corresponding to $f$ is the set $\{(x, f(x)) \mid x \in A\} \subseteq A \times B$, which we usually also call $f$. The space of relations is strictly larger than the space of functions: for example, the relation \[
    \{(T,T),(T,F),(F,T),(F,F)\} : \mathsf{Bool} \leftrightarrow \mathsf{Bool}
\] cannot be represented as a function $f : \mathsf{Bool} \to \mathsf{Bool}$, because both $T$ and $F$ are sent to both $T$ and $F$. In fact, this particular relation is the same as $\mathsf{Bool} \times \mathsf{Bool}$ and is called the full relation on $\mathsf{Bool}$.

It is very easy to draw pictures of relations. Below are pictures of two relations $R : X \leftrightarrow Y$ and $S : A \times B \leftrightarrow C \times D$. We call the left side of the relation the domain, and the right side the codomain or range. When we draw relations that have a pair at both the domain and codomain, we can choose to draw the relation as four-sided; this is useful for circuit design. Note that we read values from the bottom upwards, and that in the four-sided case the domain is $\left<left,top\right>$ and the codomain is $\left<bottom,right\right>$.

\begin{figure}[htpb]
    \centering
    % R
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \draw (1, 0) rectangle (3, 2);
            \node at (2, 1) (R) {$R$};
            \node at (0, 1) (x) {$x$};
            \node at (4, 1) (y) {$y$};
            \draw (x) -- (1, 1);
            \draw (3, 1) -- (y);
        \end{tikzpicture}
    \end{subfigure}
    % S (2-sided)
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \node at (2, 1) (S) {$S$};
            \node at (0, 0.65) (a) {$a$};
            \node at (0, 1.35) (b) {$b$};
            \node at (4, 0.65) (c) {$c$};
            \node at (4, 1.35) (d) {$d$};
            \draw (1, 0) rectangle (3, 2);
            \draw (a) -- (1, 0.65);
            \draw (b) -- (1, 1.35);
            \draw (c) -- (3, 0.65);
            \draw (d) -- (3, 1.35);
        \end{tikzpicture}
    \end{subfigure}
    % S (4-sided)
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \node at (2, 2) (S) {$S$};
            \node at (0, 2) (a') {$a$};
            \node at (2, 4) (b') {$b$};
            \node at (2, 0) (c') {$c$};
            \node at (4, 2) (d') {$d$};
            \draw (1, 1) rectangle (3, 3);
            \draw (a') -- (1, 2);
            \draw (b') -- (2, 3);
            \draw (c') -- (2, 1);
            \draw (d') -- (3, 2);
        \end{tikzpicture}
    \end{subfigure}
    \caption{A drawing of $R : X \leftrightarrow Y$ and two different drawings of $S : A \times B \leftrightarrow C \times D$.}
    \Description
        [A drawing of a block "R", and two different drawings of a block "S".]
        {Three drawings. The first is of a block "R" with one input on the domain (left) and one output on the codomain (right). The second is of a block "S" with two inputs on the domain and codomain. The third is of "S" again, but the domain wires have moved so one is on the left and one is coming from the top, and the codomain wires have also moved so that one is coming from the bottom and one from the right.}
    \label{fig:relation}
\end{figure}

As it turns out, drawing pictures for relations is not just a visual exercise: there is a formal way of converting between the syntax of relational programs and pictures \cite{BrownAllegories94}.

There are three foundational operators defined on relations, which form the basic combinators for the language \emph{Ruby}:

\begin{align*}
    (-;-) &: (A \leftrightarrow B) \to (B \leftrightarrow C) \to (A \leftrightarrow C), \\
    [-,-] &: (A \leftrightarrow B) \to (C \leftrightarrow D) \to (A \times C \leftrightarrow B \times D), \\
    (-)^{-1} &: (A \leftrightarrow B) \to (B \leftrightarrow A)
.\end{align*}

The first $(-;-)$ is for sequential composition and has the following definition: \[
    x(R;S)y \iff \exists p\ :\ xRp \wedge pSy
.\] This is the most tricky of the three combinators to actually implement, due to the fact that the value of $z$ must often be inferred; we will revisit this issue later. It can be drawn as follows:
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture} 
        \node at (-0.5, 1) (x) {$x$};
        \node at (2, 1) (R) {$R$};
        \node at (4, 1.2) (p) {$p$};
        \node at (6, 1) (S) {$S$};
        \node at (8.5, 1) (y) {$y$};

        \draw (x) -- (1, 1);
        \draw (1, 0) rectangle (3, 2);
        \draw (3, 1) -- (5, 1);
        \draw (5, 0) rectangle (7, 2);
        \draw (7, 1) -- (y);

        \node at (4, 2.2) (RS) {$R;S$};
        \draw[dashed] (0.5, -0.5) rectangle (7.5, 2.5);
    \end{tikzpicture}
    \caption{Sequential composition of relations.}
    \Description
        [A drawing of two relations sequentially composed together.]
        {A drawing of the sequential composition of two relations. The codomain wire of the first block is connected to the domain wire of the second block.}
    \label{fig:rel_seq}
\end{figure}

The second is for parallel composition and is defined by \[
    (x,a)[R,S](y,b) \iff xRy \wedge aSb
.\]
\begin{figure}[htpb]
    \centering
    % Par
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{tikzpicture}
            \node at (-0.5, 1) (x) {$x$};
            \node at (2, 1) (R) {$R$};
            \node at (4.5, 1) (y) {$y$};
    
            \node at (-0.5, 3.5) (a) {$a$};
            \node at (2, 3.5) (S) {$S$};
            \node at (4.5, 3.5) (b) {$b$};
            
            \draw (x) -- (1, 1);
            \draw (1, 0) rectangle (3, 2);
            \draw (3, 1) -- (y);
    
            \draw (a) -- (1, 3.5);
            \draw (1, 2.5) rectangle (3, 4.5);
            \draw (3, 3.5) -- (b);
    
            \node at (2, 5) (RS) {$[R,S]$};
            \draw[dashed] (0.5, -0.5) rectangle (3.5, 5.5);
        \end{tikzpicture}
    \end{subfigure}
    % Inv
    \begin{subfigure}[c]{0.45\textwidth}
        % Inv (bend)
        \begin{subfigure}{0.9\textwidth}
            \begin{tikzpicture}
                \draw (1, 0) rectangle (3, 2);
                \node at (2, 1) (R) {$R^{-1}$};
                \node at (-0.2, 1) (x) {$x$};
                \node at (4.5, 1) (y) {$y$};

                \draw (x) -- (0.3, 1) -- (0.3, 2.5) -- (3.3, 2.5) -- (3.3, 1) -- (3, 1);
                \draw (y) -- (3.7, 1) -- (3.7, -0.5) -- (0.7, -0.5) -- (0.7, 1) -- (1, 1);
            \end{tikzpicture}
        \end{subfigure}
        
        \bigbreak
        \bigbreak
        
        % Inv (no bend)
        \begin{subfigure}{0.9\textwidth}
            \begin{tikzpicture}
                \draw (1, 0) rectangle (3, 2);
                \node at (2, 1) (R) {$R^{-1}$};
                \node at (0, 1) (y) {$y$};
                \node at (4, 1) (x) {$x$};
                \draw (1, 1) -- (y);
                \draw (x) -- (3, 1);
            \end{tikzpicture}
        \end{subfigure}
    \end{subfigure}

    \caption{Parallel composition of relations, and two ways of drawing the inverse of a relation.}
    \Description
        [A drawing of parallel composition, and two different drawings of relational inverse.]
        {Three drawings. The first is of parallel composition of two blocks; they are placed vertically above one another, with no wires connecting. The second is a drawing of the inverse of a block - the wire from the left is wrapped around the block to connect to its right-hand side, and similarly the wire from the right is wrapped around to connect to its left-hand side. The third is the inverse again, except rather than twisting the wires, the block is simply reflected along the off-diagonal.}
    \label{fig:rel_par_inv}
\end{figure}

The third is called the inverse or converse, and is defined by \[
    xR^{-1}y \iff yRx
.\] Unlike functions, all relations are invertible. You can draw them in two ways - by bending the wires or just starting them from the opposite sides to usual. Along with these combinators, there are a group of very useful polymorphic relations that we call ``rewiring'' relations. Here are the five most useful ones:
\begin{align*}
    id &: A \leftrightarrow A & x\ &id\ y, \\
    fork &: A \leftrightarrow A \times A & x\ &fork\ \left<x,x\right>, \\
    \pi_{1} &: A \times B \leftrightarrow A & \left<x,y\right>\ &\pi_{1}\ x, \\
    \pi_{2} &: A \times B \leftrightarrow B & \left<x,y\right>\ &\pi_{2}\ y, \\
    rsh &: A \times (B \times C) \leftrightarrow (A \times B) \times C & \left<x,\left<y,z\right>\right>\ &lsh\ \left<\left<x,y\right>,z\right>
.\end{align*}

We use angle brackets $\left<-,-\right>$ for products because we are working in the category of tuples over some carrier $C$. We will be implementing these tuples as rose trees.

To combine all these concepts together, here is a small relation that we will be using in the future. We call it $fork2$, and it is defined by
\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{align*}
            &fork2 = \pi_{1}^{-1} ; [fork^{-1}, fork] ; rsh \\
            &\implies \left<x,x\right> fork2 \left< \left< x,y\right>,y\right>
        .\end{align*}        
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{tikzpicture}
            \node at (0, 2) (x1) {$x$};
            \node at (2, 4) (x2) {$x$};
            \node at (1.65, 0) (x3) {$x$};
            \node at (2.35, 0) (y1) {$y$};
            \node at (4, 2) (y2) {$y$};
    
            \node at (2, 2) (fork2) {$fork2$};
            \draw (1, 1) rectangle (3, 3);
            \draw (x1) -- (1, 2);
            \draw (x2) -- (2, 3);
            \draw (1.65, 1) -- (x3);
            \draw (2.35, 1) -- (y1);
            \draw (3, 2) -- (y2);
        \end{tikzpicture}
    \end{subfigure}
    \caption{The definition of $fork2$ and a picture of it.}
    \Description
        [The defintion of fork2 and a drawing of it as a four-sided relation.]
        {The defintion of fork2, which is pi one inverse composed with the parallel of fork inverse and fork, all composed with right shift. A diagram accompanies, showing fork2 as a four-sided relation. The relation it satisfies is that, "x" "x", is related to, "x" "y", "y". }
    \label{fig:fork2}
\end{figure}


\section{Algebraic Effects}
\label{sec:effects}

Algebraic effects are a new technique for developing DSLs that is gaining more traction in functional programming. Here we will briefly explain the motivation for algebraic effects and how they are implemented. Our implementation will be in Idris 2, but most of the code in this section is, with minor modifications, valid Haskell.

Suppose we want to develop a DSL for some simple expression language that has two operations, addition and negation, and we want this expression language to be polymorphic in the type of vaules it operates on. The simplest way to develop such a DSL would be by defining the type of syntax trees as follows:

%format Expr_1
%format Val_1
%format Add_1
%format Neg_1

\begin{code}
data Expr_1 a = Val_1 a | Add_1 (Expr_1 a) (Expr_1 a) | Neg_1 (Expr_1 a)    
\end{code}

A term in this language looks as you would expect:

%format term_1

\begin{code}
term_1 : Expr_1 Int
term_1 = Add_1 (Neg_1 (Val_1 2)) (Val_1 3)
\end{code}

An evaluation function is needed to interpret this language, and is simple enough to write just by recursing over the structure of the tree:

%format eval_1

\begin{code}
eval_1 : Expr_1 Int -> Int
eval_1 (Val_1 x) = x
eval_1 (Add_1 x y) = eval_1 x + eval_1 y
eval_1 (Neg_1 x) = -(eval_1 x)
\end{code}

Algebraic effects are similar, except that explicit recursion is replaced by implicit recursion over some continuation parameter |k|:

%format Expr_2
%format Add_2
%format Val_2
%format Neg_2
%format term_2

\begin{code}
data Expr_2 a k = Val_2 a | Add_2 k k | Neg_2 k

data Fix : (Type -> Type) -> Type where
    In:  f (Fix f) -> Fix f

term_2 : Fix (Expr_2 Int)
term_2 = In (Add_2 (In (Neg_2 (In (Val_2 2)))) (In (Val_2 3)))
\end{code}

The |Fix| datatype takes a signature |f| that has children of parameter |k| and sets |k = Fix f|, which causes recursion. If |f| is a functor, an interpretation is given quite easily through a function known as an algebra:

%format fold_fix
%format alg_2

\begin{code}
Functor (Expr_2 a) where
    map f (Val_2 a) = Val_2 a
    map f (Add_2 x y) = Add_2 (f x) (f y)
    map f (Neg_2 x) = Neg_2 (f x)

alg_2 : Expr_2 Int Int -> Int
alg_2 (Val_2 x) = x
alg_2 (Add_2 x y) = x + y
alg_2 (Neg_2 x) = -x

fold_fix : Functor f => (f a -> a) -> Fix f -> a
fold_fix alg (In x) = alg (map (fold_fix alg) x)
\end{code}

|fold_fix| applies the algebra |alg| recursively through the syntax tree, folding it into a single value. We call such a thing a \emph{handler}:

%format handler_2

\begin{code}
handler_2 : Fix (Expr_2 Int) -> Int
handler_2 = fold_fix alg_2
\end{code}

This seems all quite redundant at the moment - we have turned one datatype into two, we require one of them to be a functor, and the terms themselves are longer as every operation has to be preceded by an |In| constructor. That is because the benefit comes when we want to combine multiple DSLs together: using this system, it is trivial to take the coproduct of two or more signatures, resulting in the ability to use operations from either DSL \cite{SwierstraDTC08}:

%format :+: = "+"

\begin{code}
infixr 1 :+:
data (:+:) : (f : Type -> Type) -> (g : Type -> Type) -> a -> Type where
    Inl : f a -> (f :+: g) a
    Inr : g a -> (f :+: g) a

(Functor f, Functor g) => Functor (f :+: g) where
    map f (Inl x) = Inl (map f x)
    map f (Inr y) = Inr (map f y)
\end{code}

The coproduct combinator |:+:| is used to combine two signatures together, and at each stage you choose whether you wil use an operation from the left signature with |Inl| or from the right with |Inr|. It's like a recursive |Either|.

For example, suppose we want another operation, say multiplication. Using the original system, we would have to add a |Mul| constructor to the |Expr1| datatype, which would break all our existing code. With this system, we can make a completely new datatype just for |Mul|, and combine it with our existing one. Firstly, we define this new datatype:

\begin{spec}
data Expr' a k : Val' a | Mul k k
\end{spec}

However, note how there is redundancy in |Expr_2| and |Expr'| - both are paramaterised by the same values (|a| and |k|) and both need a |Val| constructor to represent plain values. This is excessive because both |Val| constuctors serve exactly the same purpose. Instead, we can factor out the leaf value constructor into |Fix|, and we call this new datatype |Free|:

\hspace{-0.3cm}\begin{minipage}[t]{0.50\textwidth}
\begin{code}
data Free : (Type -> Type) -> Type -> Type
    where
    Var : a -> Free f a
    Op : f (Free f a) -> Free f a
\end{code}
\begin{code}
Functor f => Functor (Free f) where
    map f (Var x) = Var (f x)
    map f (Op op) = Op (map (map f) op)
\end{code}
\end{minipage}
\vline\hspace{0.04cm}
\begin{minipage}[t]{0.49\textwidth}
\begin{code}
Functor f => Applicative (Free f) where
    pure = Var
    (Var f) <*> x = map f x
    (Op op) <*> x = Op (map (<*>x) op)
\end{code}
\begin{code}
Functor f => Monad (Free f) where
    (Var x) >>= f = f x
    (Op op) >>= f = Op (map (>>=f) op)
\end{code}
\end{minipage}

\hfill\break

|Free f a| is precisely the type of free monads over the functor |f| with carrier |a|. We can rewrite |Expr_2| to use |Free| as follows:

%format Expr_3
%format Add_3
%format Neg_3
%format term_3

\begin{code}
data Expr_3 k = Add_3 k k | Neg_3 k

Functor Expr_3  where
    map f (Add_3 x y) = Add_3 (f x) (f y)
    map f (Neg_3 x) = Neg_3 (f x)

term_3 : Free Expr_3 Int
term_3 = Op (Add_3 (Op (Neg_3 (Var 2))) (Var 3))
\end{code}

Folding a |Free| requires an algebra for the recursive case, and a generator for the leaf case:

%format gen_3
%format alg_3

\begin{code}
gen_3 : Int -> Int
gen_3 = id

alg_3 : Expr_3 Int -> Int
alg_3 (Add_3 x y) = x + y
alg_3 (Neg_3 x) = -x
\end{code}

Folding is done almost identically to |Fix|, we just have to handle the |Var| case with the generator:

%format \_ = "\lambda \anonymous"
%format handler_3

\begin{code}
fold : Functor f => (a -> b) -> (f b -> b) -> (Free f a -> b)
fold gen _ (Var x) = gen x
fold gen alg (Op op) = alg (map (fold gen alg) op)
    
handler_3 : Free Expr_3 Int -> Int
handler_3 = fold gen_3 alg_3
\end{code}

The new datatype with |Mul| no longer needs a |Val'| constructor as it's been absorbed into |Free|, and the algebra is simple:

%format Expr_4
%format alg_4

\begin{code}
data Expr_4 k = Mul k k

Functor Expr_4 where
    map f (Mul x y) = Mul (f x) (f y)

alg_4 : Expr_4 Int -> Int
alg_4 (Mul x y) = x * y
\end{code}

%format term_34

For example, a term using both languages would look like

\begin{code}
term_34 : Free (Expr_3 :+: Expr_4) Int
term_34 = Op (Inl (Add_3 (Var 3) (Op (Inr (Mul (Var 2) (Op (Inl (Neg_3 (Var 4)))))))))
\end{code}

Notice how we go left for |Add| and |Neg|, right for |Mul|, and the |Var| case is part of |Free| so doesn't need to go either left or right.

Since both languages will share the same carrier |a|, the generator will also be shared between them, but the algebras must be combined to work on their coproduct.

%format </> = "\nabla"
%format alg_f
%format alg_g

\begin{code}
infixr 1 </>
(</>) : (f a -> a) -> (g a -> a) -> (f :+: g) a -> a
(alg_f </> _) (Inl x) = alg_f x
(_ </> alg_g) (Inr y) = alg_g y
\end{code}

The final handler that can handle both languages simultaneously is then

%format handler_34

\begin{code}
handler_34 : Free (Expr_3 :+: Expr_4) Int -> Int
handler_34 = fold gen_3 (alg_3 </> alg_4)
\end{code}


\section{Ruby as an embedded DSL}
\label{sec:dsl}

In this section we will explore the various ways of embedding Ruby into Idris. The goal is to eventually have a DSL that represents exactly the valid Ruby programs. What ``valid'' actually means is something we will refine as we go, which means we will be producing a sequence of DSLs, each more restrictive than the last, and translators that can go from one step to the next (if the program is valid).

\subsection{Untyped Ruby}

The most basic embedding of Ruby is as a completely untyped language. This means that as long as the terms are syntactically valid, they typecheck. In other words, this DSL has no built-in semantics: it is precisely the words of the language with operators $(-;-)$, $[-,-]$ and $(-)^{-1}$.

%format gen_delay
%format alg_delay

\iffalse
\begin{code}
namespace Untyped
\end{code}
\fi

\begin{code}
    public export
    data RComb k = Seq k k | Par k k | Inv k
\end{code}
\begin{code}
Functor RComb where
    map f (Seq x y) = Seq (f x) (f y)
    map f (Par x y) = Par (f x) (f y)
    map f (Inv x) = Inv (f x)

Ruby : Type -> Type
Ruby = Free RComb
\end{code}

For example, our $fork2$ program could be written as

%format pi_1

\iffalse
\begin{code}
namespace Untyped
\end{code}
\fi

\begin{code}
    fork2 : Ruby String
    fork2 = Op (Seq (Op (Inv (Var "pi1")))
            (Op (Seq (Op (Par (Op (Inv (Var "fork"))) (Var "fork"))) (Var "rsh"))))
\end{code}

Writing a handler for this DSL is straightforward. For example, suppose we would like to apply nonstandard analysis to calculate the length of the longest path through the circuit. Intuitively, the semantics would be given by the equations

\begin{align*}
    \delay(Q;R) &= \delay(Q) + \delay(R) \\
    \delay([Q,R]) &= \max(\delay(Q),\delay(R)) \\
    \delay(Q^{-1}) &= \delay(Q)
.\end{align*}

Assuming that all primitive blocks have a delay of one cycle (a terrible assumption but sufficient for this example), the analysis would be given as a handler as follows:

\begin{code}
analyseDelay : Ruby a -> Nat
analyseDelay = fold gen_delay alg_delay where
    gen_delay : a -> Nat
    gen_delay = const 1

    alg_delay : RComb Nat -> Nat
    alg_delay (Seq x y) = x + y
    alg_delay (Par x y) = max x y
    alg_delay (Inv x) = x
\end{code}

We could, of course, change the type of the program to something like |Ruby (String, Nat)| where the second element of the pair is the delay. The generator would then be |gen_delay = snd|. Or we could store a lookup table of primitives and information about them, such as delay, and pass it to the generator to use during the fold.

\iffalse
\begin{code}
Show (RComb String) where
    show (Seq x y) = x ++ " ; " ++ y
    show (Par x y) = "[" ++ x ++ ", " ++ y ++ "]"
    show (Inv x) = "inv " ++ x

[freeStr] Functor f => Show (f String) => Show (Free f String) where
    show (Var x) = x
    show (Op x) = show (map show x)

Show a => Functor f => Show (f String) => Show (Free f a) where
    show (Var x) = show x
    show (Op x) = show (map show x)
\end{code}
\fi

\subsection{Typed Ruby, by signal shape}

Now we would like to start making sure Ruby programs make some semantic sense. The first thing to check is that wires are connected together properly. Let $a,b,\ldots$ range over tuples of wires denoted by angled brackets, and let $W$ denote a single wire. For example, the tuple $\left<a,W\right>$ is a pair whose first element is any tuple, and whose second element is a single wire. We therefore have the following typing rules:

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \hypo{R : b \leftrightarrow c}
            \infer2[Seq]{(Q;R) : a \leftrightarrow c}
        \end{prooftree}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \hypo{R : c \leftrightarrow d}
            \infer2[Par]{[Q,R] : \left<a, c\right> \leftrightarrow \left<b, d\right>}
        \end{prooftree}
    \end{subfigure}

    \bigbreak

    \begin{subfigure}[c]{\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \infer1[Inv]{Q^{-1} : b \leftrightarrow a}
        \end{prooftree}
    \end{subfigure}
    \caption{Typing rules for Seq, Par and Inv.}
    \Description
        [Hoare logic type inference rules for Seq, Par and Inv.]
        {Hoare logic type inference rules for Seq, Par and Inv. Seq requires that the codomain of the first block matches with the domain of the second. Par pairs the domains and codomains of its constituent blocks. Inv swaps the domain and codomain of the block.}
    \label{fig:typing1}
\end{figure}

This prevents a block that has two wires on the codomain from connecting to a block with three wires in its domain, for example.

Looking back at \cref{fig:fork2}, we can see that the type of |fork2| is two signals on the domain and three on the codomain. The signals are completely polymorphic (i.e. $x$ and $y$ are independent and could be any number of wires in any arrangement). A derivation of the type of |fork2| follows:

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{\pi_1 &: \left<\left<a,a\right>,a\right> \leftrightarrow \left<a,a\right>}
        \infer1[Inv]{\pi_1^{-1} &: \left<a,a\right> \leftrightarrow \left<\left<a,a\right>,a\right>}
        \hypo{fork &: a \leftrightarrow \left<a,a\right>}
        \infer1[Inv]{fork^{-1} &: \left<a,a\right> \leftrightarrow a}
        \hypo{fork : b \leftrightarrow \left<b,b\right>}
        \infer2[Par]{[fork^{-1}, fork] : \left<\left<a,a\right>,b\right> \leftrightarrow \left<a,\left<b,b\right>\right>}
        \infer2[Seq]{\pi_1^{-1}\ ;\ [fork^{-1}, fork] : \left<a,a\right> &\leftrightarrow \left<a,\left<b,b\right>\right>}
        \hypo{\hspace{-4.5cm}rsh : \left<a,\left<b,b\right>\right> \leftrightarrow \left<\left<a,b\right>,b\right>}
        \infer2[Seq]{\pi_1^{-1}\ ;\ [fork^{-1}, fork]\ ;\ rsh : \left<a,a\right> \leftrightarrow \left<\left<a,b\right>,b\right>}
    \end{prooftree}
    \caption{A derivation of $fork2 : \left<a,a\right> \leftrightarrow \left<\left<a,b\right>,b\right>$.}
    \Description
        [A derivation tree for the type of fork2.]
        {A derivation tree for the type of fork2, which has type, "a" "a", to "a" "b", "b".}
    \label{fig:fork2deriv}
\end{figure}

Here we have implicitly used the polymorphism of $fork$, $rsh$ and $\pi_{1}$ to instantiate them to the correct types; we will revisit polymorphism later and make the typing rules for it explicit. For now, since we will be embedding this into Idris' type system, implicit polymorphism in derivations is fine.

Now we need a way to represent these types. Since they are tuples of arbitrary size and depth, we will be using rose trees:

\begin{code}
data Rose a = V a | T (Vect n (Rose a))
\end{code}

We make |Rose| an instance of |Functor|, |Show|, |Foldable|, |Traversable| and |Eq|. Furthermore, we can overload the |Nil| and |(::)| operators to use list notation when constructing Rose trees:

\begin{code}
namespace RoseSpace
    public export
    (::) : Rose a -> Vect n (Rose a) -> Rose a
    x :: xs = T (x :: xs)
    public export
    Nil : Rose a
    Nil = T []
\end{code}

This allows us to write |[x, [y, z]]| rather than |T [x, T [y, z]]|. However, since these are functions and not constructors, we still need to use |T| when pattern matching.

\iffalse
\begin{code}
Show a => Show (Rose a) where
    show (V x) = show x
    show (T rs) = show rs

Functor Rose where
    map f (V x) = V (f x)
    map f (T xs) = T $ map (map f) xs

Foldable Rose where
    foldr f a (V x) = f x a
    foldr f a (T (x :: xs)) = foldr f (foldr f a (T xs)) x
    foldr f a (T []) = a

Traversable Rose where
    traverse f (V x) = map V (f x)
    traverse f (T xs) = pure T <*> traverse (traverse f) xs

vectEq : Eq a => (xs : Vect n a) -> (ys : Vect m a) -> Bool
vectEq (x::xs) (y::ys) = x == y && vectEq xs ys
vectEq [] [] = True
vectEq _ _ = False

Eq a => Eq (Rose a) where
    V x == V y = x == y
    (T xs) == (T ys) = vectEq xs ys
    _ == _ = False
\end{code}
\fi

We choose to use the length-indexed |Vect| type over the non-indexed |List| type because knowing the length statically will help us later when constructing various objects that rely on it.

Strictly speaking, in hardware every single wire is of the same ``type'' - it s a single wire that can carry a single boolean value (ignoring the possibility of high impedance) therefore their values should be represented with an Idris ``Bool''. However, this would be extremely inefficient when simulating anything with large buses, say 32- or 64-bit wide buses. Therefore it would be better to allow for tuples to be of arbitrary type.

Unfortunately, due to reasons that will become clear later, we cannot simply use Rose trees over |Type|. We are required to construct our own universe of types:

%format type = "\Varid{type}"

\begin{minipage}[t]{0.48\textwidth}
\begin{code}
data Typ = TInt | TBool | TUnit
\end{code}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\begin{code}
type : Typ -> Type
type TInt = Int
type TBool = Bool
type TUnit = Unit
\end{code}
\end{minipage}

For ease of use, we also define the following aliases:

\begin{minipage}[t]{0.48\textwidth}
\begin{code}
TShp : Type
TShp = Rose Typ
\end{code}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\begin{code}
TShp' : Type
TShp' = (TShp, TShp)
\end{code}
\end{minipage}

The types of relations are then of the form |TShp'|.

\subsubsection{Indexed functors and monads}

Indexed datatypes allow us to embed semantic data about our datatypes at the type-level, meaning they are checked at compile-time - our programs are guaranteed to be correct by construction \cite{McBrideKleisli11}. Since we were using functors and the free monad for untyped Ruby, it stands to reason that typed Ruby will use indexed functors and the indexed free monad.

An indexed functor is a functor over the category $a \to Type$ where $a \in Type$. Ideally, a functor $F$ would be specified as being over some category $\mathcal{C}$, which means both indexed and non-indexed functors are represented and implemented in exactly the same way; unfortunately, no programming language to date has this feature. Therefore we will have to make do by defining a new typeclass to represent indexed functors:

\begin{code}
interface IFunctor (h : (a -> Type) -> a -> Type) where
    imap : {f, g : a -> Type} -> (forall x . f x -> g x) -> (forall x . h f x -> h g x)
\end{code}

An indexed functor |h| takes an indexer |k : a -> Type| and an index |t : a|. The type of leaves is not |a| but rather |k p| for values |p : a|. This means each leaf (and indeed each subtree) can be of a different type, which is dictated by the indexer |k|. The indexed map, |imap|, can't just take a function |a -> b| because |a| and |b| are unknown as they are determined by the indexer. Therefore it takes a function |f x -> g x| that works on \emph{all} |x|, and lifts it to act on |h f|.

For now, we will be setting |k = Const t| for some type |t|. This means that all leaves and subtrees will be of the same type, which is no different than the usual non-indexed version. The crucial difference is that we will still be indexed, and those indices will exist at the type level and at compile-time, rather than at runtime.

Here are our Ruby combinators expressed as an indexed functor:

\iffalse
\begin{code}
namespace Typed
\end{code}
\fi

\begin{code}
    public export
    data TComb : (k : TShp' -> Type) -> TShp' -> Type where
        Seq : k (a, b) -> k (b, c) -> TComb k (a, c)
        Par : k (a, b) -> k (c, d) -> TComb k ([a, c], [b, d])
        Inv : k (a, b) -> TComb k (b, a)
\end{code}
\begin{code}
IFunctor TComb where
    imap f (Seq q r) = Seq (f q) (f r)
    imap f (Par q r) = Par (f q) (f r)
    imap f (Inv q) = Inv (f q)
\end{code}

Our programs, and all subprograms, are now directly indexed by their types.Of course we now need to make an indexed version of |Monad| as well:

\begin{code}
interface IFunctor m => IMonad (a : Type) (m : (a -> Type) -> a -> Type) where
    skip : {f : a -> Type} -> (forall x . f x -> m f x)
    extend : {f, g : a -> Type} -> (forall x . f x -> m g x) -> (forall x . m f x -> m g x)
\end{code}

\iffalse
\begin{code}
(>>=) : {a : Type} -> {m : (a -> Type) -> a -> Type} -> IMonad a m => {f, g : a -> Type}
        -> (forall x . m f x -> (forall y . f y -> m g y) -> m g x)
(v >>= k) = extend {f} k v

pure : {a : Type} -> {m : (a -> Type) -> a -> Type} -> IMonad a m => {f : a -> Type}
    -> (forall x . f x -> m f x)
pure = skip {f}
\end{code}
\fi

We also overload the |>>=| and |pure| operators to work with |IMonad| rather than |Monad|. Our implementation of |IFree| is straightforward:

\begin{code}
data IFree : (f : (a -> Type) -> a -> Type) -> (c : a -> Type) -> a -> Type where
    Ret : forall x . c x -> IFree f c x
    Do : forall x . f (IFree f c) x -> IFree f c x

{a : Type} -> {h : (a -> Type) -> a -> Type} -> IFunctor h => IFunctor (IFree h) where
    imap k (Ret x) = Ret (k x)
    imap k (Do op) = Do (imap {f=IFree h f} (imap {f} k) op)

{a : Type} -> {h : (a -> Type) -> a -> Type} -> IFunctor h => IMonad a (IFree h) where
    skip x = Ret x
    extend k (Ret x) = k x
    extend k (Do op) = Do (imap {f=IFree h f} (extend {m=IFree h} k) op)
\end{code}

\iffalse
\begin{code}
ifold :  {f : (a -> Type) -> (a -> Type)} -> IFunctor f =>
    {c : a -> Type} -> {d : a -> Type} -> (forall x . c x -> d x) ->
    (forall x . f d x -> d x) ->
    (forall x . IFree f c x -> d x)
ifold gen _ (Ret x) = gen x
ifold gen alg (Do op) = alg (imap {f=IFree f c} {g=d} (ifold gen alg) op)
\end{code}
\fi

Sometimes the typechecker gets a bit lost so we have to help it along by manually passing |f| and |g| to |imap| and |extend|, but since we've done this here it won't be necessary to do it anywhere else. Note that we skip defining the |IApplicative|. Indexed applicatives are more tricky and don't exist in general \cite{AtkeyParameterised09}, however we won't need them here so we skip their implementation.

As mentioned already, for now we won't take advantage of the special feature of indexing that allows the type of the subtrees to vary; rather, we'll be using a constant indexer:

\begin{code}
Const : Type -> (x : Type) -> x -> Type
Const t _ _ = t
\end{code}

\iffalse
\begin{code}
namespace Typed
\end{code}
\fi

Rather than writing |Ret( ... (Ret ...))| everywhere, we define smart constructors and some custom operators to go with them:

%format <:> = "\,;"
%format <|> = "\,\vert\,"

\begin{code}
    infixl 1 <:>
    (<:>) : IFree TComb k (a, b) -> IFree TComb k (b, c) -> IFree TComb k (a, c)
    (q <:> r) = Do (Seq q r)

    infixl 1 <|>
    (<|>) : IFree TComb k (a, b) -> IFree TComb k (c, d) -> IFree TComb k (T [a, c], T [b, d])
    (q <|> r) = Do (Par q r)

    inv : IFree TComb k (a, b) -> IFree TComb k (b, a)
    inv q = Do (Inv q)
\end{code}

The code for |fork2| is now

\begin{code}
    TRuby : Type -> TShp' -> Type
    TRuby a = IFree TComb (Const a TShp')

    build : t -> (x : TShp') -> TRuby t x
    build v _ = Ret v

    fork : {u : TShp} -> TRuby String (u, [u, u])
    fork = build "fork" (u, T [u, u])

    pi_1 : {u, v : TShp} -> TRuby String ([u, v], u)
    pi_1 = build "pi1" (T [u, v], u)

    rsh : {u, v, w : TShp} -> TRuby String ([u, [v, w]], [[u, v], w])
    rsh = build "rsh" ([u, [v, w]], [[u, v], w])

    fork2 : {u, v : TShp} -> TRuby String ([u, u], [[u, v], v])
    fork2 = inv pi_1 <:> (inv fork <|> fork) <:> rsh
\end{code}

At this point, we are well on our way to writing type-safe Ruby as a simple embedded language. However, there are additional considerations to take into account.


\subsection{Typed Ruby, by signal direction}

Consider for a moment the actual purpose of Ruby: to describe hardware. Unfortunately (or perhaps fortunately, if you dislike relational programming) hardware is not relational, it is functional. As mentioned, functions are a strict subset of relations. It therefore follows that Ruby as we have implemented it can describe impossible circuits. For example, if $add$ is defined by $\left<x,y\right>\ add\ (x+y)$ then the circuit $add\ ;\ add^{-1}$ expresses the relation \[
    \left<x,y\right> (add\ ;\ add^{-1}) \left<a,b\right> \iff x + y = a + b
.\] However, as a circuit, an adder has data flowing from domain to codomain: $x$ and $y$ are inputs, and $x + y$ is the output. The below picture makes the problem obvious: the wire connecting the left and right adders is being driven twice, which would be completely rejected by a synthesis tool, and even if it were accepted would result in completely undefined behaviour (the relational behaviour would certainly not be preserved). It would therefore be useful if we could index our syntax tree by both the signal structure and signal direction.

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}[>={Stealth[inset=0pt,length=8pt,angle'=40]}]
        \node at (0, 0.65) (x) {$x$};
        \node at (0, 1.35) (y) {$y$};
        \node at (2, 1) (add1) {$add$};
        \node at (4.5, 1.4) (sum) {$x + y = a + b$};
        \node at (7, 1) (add2) {$add^{-1}$};
        \node at (9, 0.65) (a) {$a$};
        \node at (9, 1.35) (b) {$b$};

        \draw[->] (x) -- (1, 0.65);
        \draw[->] (y) -- (1, 1.35);
        \draw (1, 0) rectangle (3, 2);
        \draw[->] (3, 1) -- (4.5, 1);
        \draw[<-] (4.5, 1) -- (6, 1);
        \draw (6, 0) rectangle (8, 2);
        \draw[<-] (8, 0.65) -- (a);
        \draw[<-] (8, 1.35) -- (b);
    \end{tikzpicture}
    \caption{A picture of $add\ ;\ add^{-1}$. Arrows represent the direction of data flow.}
    \Description
        [A picture of add composed with add inverse, with arrows representing the direction of data flow.]
        {A picture of add composed with add inverse. There are arrows on the domain of the first add pointing towards the add, and arrows on the codomain of the second add again pointing inwards towards the add. The "output" of the two adds meet in the middle, and both adds are driving the same wire which is symbolised by the arrows pointing against each other and clashing.}
    \label{fig:addfail}
\end{figure}

This new information requires an update to our typing rules. Earlier, we used $A,B,\ldots$ to vary over empty tuples, which denote the structure of the domain and codomain. Now we will vary over tuples of directions. There are two ways to set this up: we can let directions be \emph{Right} and \emph{Left} (i.e. arrow pointing to the right or to the left), or \emph{In} and \emph{Out} (i.e. arrows entering the block or leaving the block). The more natural choice is to use \emph{In} and \emph{Out}, since the information of being an input or output is more important that the exact direction of data flow. It is also symmetric - inputs and outputs don't change under inverse.

However, in practice we found it significantly easier to use \emph{Left} and \emph{Right} when actually implementing primitives. They are also equal when it comes to typing rules: \emph{In} and \emph{Out} would change our typing rule for |Seq| whereas \emph{Left} and \emph{Right} would change it for |Inv|. Neither are easier or harder to use than the other, although once again in practice |Seq| is used more than |Inv| therefore we ought to prioritise an easier typing rule for it.

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \infer1[Inv]{Q^{-1} : \overline{b} \leftrightarrow \overline{a}}
        \end{prooftree}
    \end{subfigure}
    where \quad \quad
    \begin{subfigure}[c]{0.4\textwidth}
        \begin{align*}
            \overline{Left} &= Right \\
            \overline{Right} &= Left \\
            \overline{\left<a_{1},\ldots,a_{n}\right>} &= \left<\overline{a_{1}},\ldots,\overline{a_{n}}\right>
        \end{align*}
    \end{subfigure}
    \caption{New typing rule for |Inv|, and definition of the complement operation.}
    \Description
        [A new typing rule for Inv, and a defintion of the complement operation.]
        {A new typing rule for inv, where if Q has type "a" to "b" then Q inverse has type "b" bar to "a" bar, where bar denotes complement. Complement is defined as: the complement of left is right, the complement of a tuple is the tuple of the complements, and complement is symmetric.}
    \label{fig:typing2}
\end{figure}

If we wish to be more mathematical, our types now live in a binary algebra, where $C(a,b)$ means that $a$ and $b$ are complementary. There is a rich type theory around this; although the theory is not practical to actually implement, it can aid in proving correctness-preserving transformations \cite{HuttonThesis92}.

When inverting a block, lefts and rights swap. The rules for |Seq| and |Par| remain unchanged.

The types of polymorphic primitives such as $fork$ have changed dramatically. We will need to define two new predicates, $\ell$ and $\rho$. $\ell(a)$ holds iff all elements of $a$ are $Left$, and likewise $\rho(a)$ holds iff all elements of $a$ are $Right$. We have the following laws:

\begin{align*}
    \ell(In) \qquad \neg \ell(Out) \qquad \ell(\left<a_{1},\ldots,a_{n}\right>) &\iff \bigwedge \ell(a_{k}) \\
    \rho(Out) \qquad \neg \rho(In) \qquad \rho(\left<a_{1},\ldots,a_{n}\right>) &\iff \bigwedge \rho(a_{k}) \\
    \ell(a) \iff \rho(\overline{a})
\end{align*}

For example, the typing rules for our three primitives $\pi_{1}$, $fork$ and $rsh$ are

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{\rho(b)}
        \infer1{\pi_1 : \left<a,b\right> \leftrightarrow a}
    \end{prooftree}
    \qquad \qquad
    \begin{prooftree}
        \hypo{\iota(b)}
        \rewrite{}
        \infer1{rsh : \left<a,\left<b,c\right>\right> \leftrightarrow \left<\left<a,b\right>,c\right>}
    \end{prooftree}
    \bigbreak
    \begin{prooftree}
        \hypo{\rho(a)}
        \infer1{fork : a \leftrightarrow \left<a,a\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\rho(a)}
        \infer1{fork : \overline{a} \leftrightarrow \left<\overline{a},a\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\rho(a)}
        \infer1{fork : \overline{a} \leftrightarrow \left<a,\overline{a}\right>}
    \end{prooftree}
    \caption{Typing rules for $\pi_{1}$, $fork$ and $rsh$.}
    \Description
        [Typing rules in this new system for pi one, fork and right shift.]
        {Typing rules for pi one, fork and right shift. Pi one requires that its second argument (the value that will be discarded) is going to the right. Right shift has no conditions. There are three typing rules for fork. The first is that "a" is going to the right, and the fork is from "a", to "a" "a". The second is that "a" is going to the right, and the fork is from "a" bar, to "a" bar "a". The third is that "a" is going to the right, and the fork is from "a" bar, to "a", "a" bar. }
    \label{fig:typingPrims}
\end{figure}

$\pi_{1}$ and $rsh$ are simple enough, but with $fork$ things start to get tricky - there are three different ways of typing a fork, because while the block is completely polymorphic in the shape of its signals, one must be an input and the other two must be outputs.

In practice, however, we would like the type to be free from functions; it may be difficult for type inference to infer information from a signature with complements, so it would be better to express complements as an explicit property $C$ where $C(a,b) \iff a = \overline{b}$. Inferring a proof for $C(a,L)$ for a type $T [a, L]$ is easier for the Idris type system than unifying $\left<a,\overline{a}\right>$ and $T [a, L]$ for instance. Our actual implemetation will therefore use the following typing rules for $fork$ and $inv$:

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{Q : a \leftrightarrow b}
        \hypo{C(a,x)}
        \hypo{C(b,y)}
        \infer3[Inv]{Q^{-1} : y \leftrightarrow x}
    \end{prooftree}
    \bigbreak
    \begin{prooftree}
        \hypo{\rho(a)}
        \infer1{fork : a \leftrightarrow \left<a,a\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\rho(a)}
        \hypo{C(a,b)}
        \infer2{fork : b \leftrightarrow \left<a,b\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\rho(a)}
        \hypo{C(a,b)}
        \infer2{fork : b \leftrightarrow \left<b,a\right>}
    \end{prooftree}
    \caption{Better typing rules for $fork$ and $invs$.}
    \Description
        [Better typing rules for fork and inv.]
        {Better typing rules for fork and inv. Wherever a bar was used previously, it has been replaced with a new variable and an additional requirement that the two variables are complementary, where see "x" "y" means "x" and "y" are complementary. The rule for inv is that if Q is from "a" to "b", see "a" "x" and see "b" "y" then Q inverse is of type "y" to "x". The first fork rule is unchanged. The second says that if "a" is going to the right and see "a" "b" then fork is of type "b" to "a" "b". The third says that if "a" is going to the right and see "a" "b" then fork is of type "b" to "b" "a".}
    \label{fig:typingPrims2}
\end{figure}

Now there are no functions on the bottom of any of these rules, only free variables that are constrained by the properties on the top.

\begin{minipage}[t]{0.32\textwidth}
\begin{code}
data Dir = Left | Right
\end{code}
\end{minipage}
\begin{minipage}[t]{0.32\textwidth}
\begin{code}
DShp, DShp' : Type
DShp = Rose (Typ, Dir)
DShp' = (DShp, DShp)
\end{code}
\end{minipage}
\begin{minipage}[t]{0.32\textwidth}
\begin{code}
L, R : (t : Typ) -> DShp
L t = V (t, Left)
R t = V (t, Right)
\end{code}
\end{minipage}

\iffalse
\begin{code}
Show Dir where
    show Left = "L"
    show Right = "R"

Eq Dir where
    Left == Left = True
    Right == Right = True
    _ == _ = False
\end{code}
\fi

\hspace{-0.4cm}\begin{minipage}[b]{0.46\textwidth}
\begin{code}
mutual
    data Lefts : (x : DShp) -> Type where
        VLefts : Lefts (L a)
        TLefts : Lefts' xs -> Lefts (T xs)
\end{code}
\end{minipage}
\vline\hspace{0.05cm}
\begin{minipage}[b]{0.53\textwidth}
\begin{code}
    data Lefts' : Vect n DShp -> Type where
        VLefts' : Lefts' []
        TLefts' : Lefts x -> Lefts' xs -> Lefts' (x :: xs)
\end{code}
\end{minipage}
    
The |mutual| block allows for mutual recursion, because Idris is usually define-before-use. We could have opted to combine both defintions together with a heterogeneous vector like |TLefts : {xs : Vect n DShp} -> HVect (map Lefts xs) -> Lefts (T xs)| but heterogeneous vectors are much harder to reason about than plain vectors, so splitting into two datatypes like this is larger syntactically but significantly simpler to use, and easier for the compiler to automatically provide implementations for. |Rights| is defined analogously for $\rho$. $C$ is defined with the same technique:

\iffalse
\begin{code}
mutual
    data Rights : DShp -> Type where
        VRights : Rights (R a)
        TRights : Rights' xs -> Rights (T xs)
    
    data Rights' : Vect n DShp -> Type where
        VRights' : Rights' []
        TRights' : Rights x -> Rights' xs -> Rights' (x :: xs)
\end{code}
\fi

\begin{code}
mutual
    data Comp : (x : DShp) -> (y : DShp) -> Type where
        [search y]
        VCompLR : Comp (L t) (R t)
        VCompRL : Comp (R t) (L t)
        TComp : Comp' xs ys -> Comp (T xs) (T ys)
    
    data Comp' : (xs : Vect n DShp) -> (ys : Vect n DShp) -> Type where
        [search ys]
        VComp' : Comp' [] []
        TComp' : Comp x y -> Comp' xs ys -> Comp' (x :: xs) (y :: ys)
\end{code}

Our implementation of |Ruby| is then

\iffalse
\begin{code}
namespace Directed
\end{code}
\fi

\begin{code}
    public export
    data DComb : (k : DShp' -> Type) -> DShp' -> Type where
        Seq : k (a, b) -> k (b, c) -> DComb k (a, c)
        Par : k (a, b) -> k (c, d) -> DComb k ([a, c], [b, d])
        Inv : {a', b' : DShp} -> Comp a a' => Comp b b' => k (a, b) -> DComb k (b', a')
\end{code}

\iffalse
\begin{code}
    public export
    IFunctor DComb where
        imap f (Seq q r) = Seq (f q) (f r)
        imap f (Par q r) = Par (f q) (f r)
        imap f (Inv q) = Inv (f q)
\end{code}
\fi

We will omit the implementations of |IFunctor DComb| and some of the primitives as they are practically identical to those for |TComb|. Here are the functions that have changed:

\iffalse
\begin{code}
    public export
    (<:>) : IFree DComb k (a, b) -> IFree DComb k (b, c) -> IFree DComb k (a, c)
    (q <:> r) = Do (Seq q r)
    
    public export
    (<|>) : IFree DComb k (a, b) -> IFree DComb k (c, d) -> IFree DComb k ([a, c], [b, d])
    (q <|> r) = Do (Par q r)
\end{code}
\fi

\begin{code}
    public export
    inv : {a', b' : DShp} -> Comp a a' => Comp b b' =>
        IFree DComb k (a, b) -> IFree DComb k (b', a')
    inv q = Do (Inv q)
\end{code}
\iffalse
\begin{code}
namespace Directed'
\end{code}
\fi
\begin{code}
    DRuby : Type -> DShp' -> Type
    DRuby a = IFree DComb (Const a DShp')

    build : t -> {x : DShp'} -> DRuby t x
    build v {x} = Ret v

    pi_1 : {x, y : DShp} -> Rights y => DRuby String ([x, y], x)
    pi_1 = build "pi1"

    rsh : {x, y, z : DShp} -> DRuby String ([x, [y, z]], [[x, y], z])
    rsh = build "rsh"
\end{code}

$fork$ becomes slightly annoying (once again): there are three ways to type it, but we want one function, not three. We combine the three different typing rules for $fork$ into a data structure as follows:

\begin{code}
data Fork : (x : DShp) -> (y : DShp) -> (z : DShp) -> Type where
    Fork1 : Rights x -> Fork x x x
    Fork2 : Comp x y -> Rights x -> Fork y y x
    Fork3 : Comp x y -> Rights x -> Fork y x y
\end{code}

The definition of $fork$ is then

\iffalse
\begin{code}
namespace Directed'
\end{code}
\fi

\begin{code}
    fork : {x, y, z : DShp} -> Fork x y z => DRuby String (x, [y, z])
    fork = build "fork"
\end{code}

There are a number of straightforward properties that need to be established for inference to work; the compiler will often need to have proofs that

\begin{itemize}
    \item If $\ell(a)$ and $C(a,b)$ then $\rho(b)$.
    \item If $\rho(a)$ and $C(a,b)$ then $\ell(b)$.
    \item If $C(a,b)$ then $C(b,a)$.
\end{itemize}

These can be implemented as functions

\begin{spec}
compLR : Comp x y -> Lefts x -> Rights y
compRL : Comp x y -> Rights y -> Lefts x
compComp : Comp x y -> Comp y x
\end{spec}

and then tagged with the |%hint| attribute so they are used automatically.

Unfortunately, even with these additions, our implementation of |fork2| does not compile:

\begin{spec}
    fork2 : {x, y, z, w : DShp} -> Lefts x => Comp x y => Rights z => Comp z w
        => DRuby String ([x, y], [[y, z], w])
    fork2 = inv pi_1 <:> (inv fork <|> fork) <:> rsh
\end{spec}

Idris has a number of limitations that make this type inference fail. Firstly, the above hinted functions cannot be chained together. We can mark an input to be searched for automatically in some cases, however in other cases this will cause a loop in the search algorithm which will cause the entire search to be aborted. For example, if we wrote the symmetry of $C$ as

\begin{spec}
compComp : Comp x y => Comp y x
\end{spec}

then the compiler would try to find a |Comp y x| by looking for a |Comp x y|, with the extra permission that it can do a proof search for the |Comp x y|. But we can find a |Comp x y| by looking for a |Comp y x| ... which puts us back where we started, and the search aborts as it's ended up in a loop.

Idris does support the idea of determining arguments - there was a hint in the definition of |Comp| that said the second argument determines the first (by experimentation this was more successful than the other way around). However, this does not help with solving constraints like |Comp [a, ?y] [?x, b]|, which requires not only that either argument determines the other, but also that the determining argument can be different at every level.

Finally, Idris has full dependent types. Dependent type inference is very much undecidable, which means there is not much motivation to implement a sophisticated and powerful type inference algorithm, especially since Idris forces the programmer to give a type signature for every function anyway. By contrast, a language such as Haskell does not have (full) dependent types, and so type inference is decidable in most non-pathological cases, therefore there is more motivation to implement a powerful type inference system. It seems likely that GHC would be able to solve some of these constraints; if not for the lack of proper support for dependent types \cite{LindleyHasochism13} I would have chosen it instead of Idris.

\iffalse
\begin{code}
mutual
    %hint
    export
    compLR : Comp x y -> Lefts x -> Rights y
    compLR VCompLR VLefts = VRights
    compLR (TComp cs) (TLefts is) = TRights (compLR' cs is)

    compLR' : Comp' xs ys -> Lefts' xs -> Rights' ys
    compLR' VComp' VLefts' = VRights'
    compLR' (TComp' a as) (TLefts' b bs) = TRights' (compLR a b) (compLR' as bs)

mutual
    %hint
    export
    compRL : Comp x y -> Rights y -> Lefts x
    compRL VCompLR VRights = VLefts
    compRL (TComp cs) (TRights is) = TLefts (compRL' cs is)

    compRL' : Comp' xs ys -> Rights' ys -> Lefts' xs
    compRL' VComp' VRights' = VLefts'
    compRL' (TComp' a as) (TRights' b bs) = TLefts' (compRL a b) (compRL' as bs)

mutual
    %hint
    export
    compComp : Comp x y -> Comp y x
    compComp VCompLR = VCompRL
    compComp VCompRL = VCompLR
    compComp (TComp cs) = TComp (compComp' cs)

    compComp' : Comp' xs ys -> Comp' ys xs
    compComp' VComp' = VComp'
    compComp' (TComp' a as) = TComp' (compComp a) (compComp' as)
\end{code}
\fi

\iffalse
\begin{code}
namespace Directed'
\end{code}
\fi

For now then, the only solution is to split more complex programs into smaller pieces:

\begin{code}
    temp : {x, y, z, w : DShp} -> Lefts x => Comp x y => Rights z => Comp z w
        => DRuby String ([[x, y], w], [[y, z], w])
    temp = (inv fork <|> fork) <:> rsh

    fork2 : {x, y, z, w : DShp} -> Lefts x => Comp x y => Rights z => Comp z w
        => DRuby String ([x, y], [[y, z], w])
    fork2 = inv pi_1 <:> temp
\end{code}


\subsection{Typed Ruby, by loops}


\section{Making relations functional}
\label{sec:causal}

We now have a DSL that enforces three important conditions when composing relations:

\begin{enumerate}
    \item No unconnected signals: Domain and codomain structure must agree.
    \item No badly driven signals: Inputs and outputs must be complementary.
    \item No feedback loops: Circuits are stable and do not have unbroken feedback loops.
\end{enumerate}

The first condition is a basic one required for the composition of relations. The second and third are the exact conditions required to ensure that the resulting relation can be represented by a function, i.e. is implementable in hardware. We call such a relation \emph{causal} \cite{HuttonThesis92}.

\subsection{Causal relations and functional interpretations}

A causal relation is a relation whose domain and codomain can be partitioned into two groups, inputs and outputs, such that the outputs can be determined from the inputs. We formalise this as follows \cite{HuttonRelations99}:

Let $Q : A \leftrightarrow B$ be a relation. $Q$ is called \emph{causal} if there exist objects $I_{L}, O_{L}, I_{R}, O_{R}$, isomorphisms \[
    i_{L} : A \cong \left<I_{L},O_{L}\right> \qquad i_{R} : B \cong \left<I_{R},O_{R}\right>
\] and a function \[
    f : \left<I_{L},I_{R}\right> \to \left<O_{L},O_{R}\right>
\] such that
\begin{align*}
    a\ Q\ b \iff &a = i_{L}^{-1} \left<\pi_{1} \circ i_{L} (a), \pi_{1}(x)\right> \\
    &b = i_{R}^{-1} \left<\pi_{1} \circ i_{R} (b), \pi_{2}(x)\right> \\
    \text{ where } &x = f \left<\pi_{1} \circ i_{L}(a), \pi_{1} \circ i_{R}(b)\right>
.\end{align*}

All that is to say, we can rearrange the domain and codomain into ins and outs, apply a function to the ins, and then undo the rearrangement to put the outs back where they should be. We call the triple $(i_{L}, i_{R}, f)$ a \emph{functional interpretation} of $Q : A \leftrightarrow B$. A relation may have many functional interpretations. For example, here are two functional interpretations of the identity relation $id : A \leftrightarrow A$:

\begin{align*}
    I_{L} = O_{R} = A, && I_{L} = O_{R} = 1, \\
    I_{R} = O_{L} = 1, && I_{R} = O_{L} = A, \\
    i_{L}(x) = \left<x,1\right>, && i_{L}(x) = \left<1,x\right>, \\
    i_{R}(x) = \left<1,x\right>, && i_{R}(x) = \left<x,1\right>, \\
    f\left<x,y\right> = \left<x,x\right>. && f\left<x,y\right> = \left<y,y\right>
.\end{align*}

The idea of a causal relation is actually more general than this, and has a purely categorical defintion in terms of categories of relations \cite{HuttonThesis92}.

\begin{figure}[htpb]
    \centering
    % https://q.uiver.app/?q=WzAsMTIsWzAsMCwiQSJdLFsyLDAsIlxcbGVmdDxJX0wsT19MXFxyaWdodD4iXSxbMiwyLCJJX0wiXSxbMCwyLCJcXGxlZnQ8SV9MLE9fTFxccmlnaHQ+Il0sWzQsMCwiXFxsZWZ0PElfUixPX1JcXHJpZ2h0PiJdLFs2LDAsIkIiXSxbNCwyLCJJX1IiXSxbNiwyLCJcXGxlZnQ8SV9SLE9fUlxccmlnaHQ+Il0sWzMsMiwiXFxsZWZ0PElfTCxJX1JcXHJpZ2h0PiJdLFszLDQsIlxcbGVmdDxPX0wsT19SXFxyaWdodD4iXSxbMCw0LCJPX0wiXSxbNiw0LCJPX1IiXSxbMCwxLCJpX0wiXSxbMSwyLCJcXHBpXzEiXSxbMywwLCJpX0xeey0xfSJdLFs0LDYsIlxccGlfMSIsMl0sWzUsNCwiaV9SIiwyXSxbNyw1LCJpX1Jeey0xfSIsMl0sWzIsOCwiXFx0aW1lcyIsMV0sWzYsOCwiXFx0aW1lcyIsMV0sWzgsOSwiZiJdLFs5LDEwLCJcXHBpXzEiXSxbOSwxMSwiXFxwaV8yIiwyXSxbMTAsMywiXFx0aW1lcyIsMV0sWzIsMywiXFx0aW1lcyIsMV0sWzYsNywiXFx0aW1lcyIsMV0sWzExLDcsIlxcdGltZXMiLDFdXQ==
\[\begin{tikzcd}
	A && {\left<I_L,O_L\right>} && {\left<I_R,O_R\right>} && B \\
	\\
	{\left<I_L,O_L\right>} && {I_L} & {\left<I_L,I_R\right>} & {I_R} && {\left<I_R,O_R\right>} \\
	\\
	{O_L} &&& {\left<O_L,O_R\right>} &&& {O_R}
	\arrow["{i_L}", from=1-1, to=1-3]
	\arrow["{\pi_1}", from=1-3, to=3-3]
	\arrow["{i_L^{-1}}", from=3-1, to=1-1]
	\arrow["{\pi_1}"', from=1-5, to=3-5]
	\arrow["{i_R}"', from=1-7, to=1-5]
	\arrow["{i_R^{-1}}"', from=3-7, to=1-7]
	\arrow["\times"{description}, from=3-3, to=3-4]
	\arrow["\times"{description}, from=3-5, to=3-4]
	\arrow["f", from=3-4, to=5-4]
	\arrow["{\pi_1}", from=5-4, to=5-1]
	\arrow["{\pi_2}"', from=5-4, to=5-7]
	\arrow["\times"{description}, from=5-1, to=3-1]
	\arrow["\times"{description}, from=3-3, to=3-1]
	\arrow["\times"{description}, from=3-5, to=3-7]
	\arrow["\times"{description}, from=5-7, to=3-7]
\end{tikzcd}\]
    \caption{Diagram showing how to rearrange the domain and codomain, apply the functional interpretation and return back to the domain and codomain.}
    \Description
        [Diagram showing how to rearrange the domain and codomain, apply the functional interpretation and return back to the domain and codomain.]
        {Diagram showing how to rearrange the domain and codomain, apply the functional interpretation and return back to the domain and codomain. The diagram looks like a commutative diagram.}
    \label{fig:iso}
\end{figure}

We can use $i_{L}$ to get the input and output on the domain, and then take the first element of that pair which is the input $I_{L}$. Likewise, we can do the same to $B$ to get $I_{R}$. We then combine them into a pair and apply our functional interpretation, yielding the outputs $O_{L}$ and $O_{R}$. Pair each of those back up with the inputs $I_{L}$ and $I_{R}$ and apply the isomorphism in reverse to get back into tuple form. The result should be that the input values on both sides are unchanged, but the output values have been populated.

To implement this, suppose we have two relations $Q : A \leftrightarrow B$ and $R : B \leftrightarrow C$ that we would like to compose, and suppose further that both of these relations are causal. That means we have the following:

\begin{itemize}
    \item Objects $I_{L},\, O_{L},\, I_{R},\, O_{R},\, I_{L}',\, O_{L}',\, I_{R}',\, O_{R}'$.
    \item Isomorphisms $i_{L} : A \cong \left<I_{L},O_{L}\right>$, $i_{R} : B \cong \left<I_{R},O_{R}\right>$, $i_{L}' : B \cong \left<I_{L}',O_{L}'\right>$, $i_{R}' : C \cong \left<I_{R}',O_{R}'\right>$.
    \item Functions $f : \left<I_{L},I_{R}\right> \to \left<O_{L},O_{R}\right>$ and $g : \left<I_{L}',I_{R}'\right> \to \left<O_{L}',O_{R}'\right>$.
\end{itemize}

Furthermore, for $Q$ and $R$ to compose we must have that the inputs and outputs on the codomain of $Q$ are complement to the inputs and outputs on the domain of $R$; that is to say, \[
    I_{R} = O_{L}',\, O_{R} = I_{L}',\, i_{R} = swap \circ i_{L}'
\] where $swap \left<x,y\right> = \left<y,x\right>$.

Cleaning up a bit, the composition looks like the diagram below, where \[
    a \in I_{L},\, b \in O_{L},\, c = w \in O_{R},\, d = x \in I_{R},\, y \in O_{R}',\, z \in I_{R}'
.\] Furthermore, $\left<b,c\right> = f \left<a,x\right>$ and $\left<x,y\right> = g \left<c,z\right>$. These are mutually recursive definitions for $b$ and $y$, the outputs of $(Q\,;R)$, in terms of $a$ and $z$, the inputs of $(Q\,;R)$. It follows then that if $Q$ and $R$ are causal, so is $(Q\,;R)$, with the isomorphisms for $A$ and $C$ being the same as those for $Q$ and $R$, and the interpretation function being
\begin{align*}
    h : \left<I_{L},I_{R}'\right> &\to \left<O_{L},O_{R}'\right>, \\
    h \left<a,z\right> &= \left<b,y\right> \text{ where } \\
    \left<b,c\right> &= f \left<a,x\right>, \\
    \left<x,y\right> &= g \left<c,z\right>
.\end{align*}

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}[>={Stealth[inset=0pt,length=8pt,angle'=40]}]
        \node at (0, 1) (a) {$a$};
        \node at (0, 2) (b) {$b$};
        \node at (4.5, 0.7) (c) {$c$};
        \node at (4.5, 2.3) (d) {$d$};
        \node at (6.5, 0.7) (w) {$w$};
        \node at (6.5, 2.3) (x) {$x$};
        \node at (11, 1) (y) {$y$};
        \node at (11, 2) (z) {$z$};

        \node at (2.5, 1.5) (q) {$Q$};
        \node at (8.5, 1.5) (r) {$R$};

        \draw (1, 0) rectangle (4, 3);
        \draw (7, 0) rectangle (10, 3);

        \draw[->] (a) -- (1, 1);
        \draw[<-] (b) -- (1, 2);
        \draw[->] (4, 1) -- (7, 1);
        \draw[<-] (4, 2) -- (7, 2);
        \draw[->] (10, 1) -- (y);
        \draw[<-] (10, 2) -- (z);
    \end{tikzpicture}
    \caption{A picture of $(Q\,;R)$.}
    \Description
        [A picture of sequential composition in its most general form, with a left wire and right wire each coming from the domain and codomain, and a left wire and right wire interfacing between the two blocks.]
        {A picture of sequential composition in its most general form. The block Q has input "a" and output "b" on the left, and input "d" and output "c" on the right. The block R has input "w" and output "x" on the left, and input "z" and output "y" on the right. "x" is connected to "d" and "c" is connected to "w" so that the two blocks communicate with each other bidirectionally.}
    \label{fig:functionalSeq}
\end{figure}

If there is no delay on the feedback path from $c$ back to itself (or equivalently, from $x$ back to itself) then this is likely to spiral off into infinite recursion, and even if it doesn't it's not a valid hardware implementation. However, if we can guarantee that there are delays then the mutual recursion is guaranteed to have a base case and the resulting circuit is well-defined.

Fortunately, it is easy to see that a) such an isomorphism always exists, and b) there is an obvious canonical one that can be automatically constructed.

Unfortunately, this leaves the user with a rather difficult problem: in order to define the interpretation function $f$, they need to satisfy a proof obligation that their function does in fact comply with the shape of the canonical isomorphism. After implementing this system in its entirety, it turned out that this requirement is completely untenable in practice. As an example of the nightmare code that this strategy produces, here is the interpretation of $\pi_{1}$:

\begin{spec}
pi_1 : {x, y, z: DShp} -> Ins y => Compl x z => Ruby a ([x, y], z)
pi_1 {x, y} with (make x) proof mkx
    _ | ((nx, mx) ** _) with (make z) proof mkz
     _ | ((nz, mz) ** _) with (make y) proof mky
      _ | ((ny, my) ** _) =
        let (pfx, pfy, pfz) = (simp mkx, simp mky, simp mkz)
            pfxy = makepf2 {x=x} {y=y} pfx pfy
            (Refl, Refl) = complSwap pfx pfz
            (Refl) = insPf pfy
        in Ret . Inter $ rewrite plusZeroRightNeutral mx in
        replace {p=(\v => Vect (nx+ny+v) a -> Vect (mx+snd(fst(make z))) a)}
            (cong fst pfz) $
        replace {p=(\v => Vect (nx+ny+mx) a -> Vect (mx+v) a)}
            (cong snd pfz)
        (\v => let (p, q) = splitAt (nx + ny) v in let (r, _) = splitAt nx p in
        rewrite plusCommutative mx nx in r ++ q)
\end{spec}

Clearly we need an easier way.


\subsection{A simpler implementation of causal relations}

Looking back at \cref{fig:functionalSeq}, under the assumption that there are no feedback loops there are three possibilities:

\begin{itemize}
    \item $w$ is connected to $x$.
    \item $d$ is connected to $c$.
    \item Neither of the above.
\end{itemize}

The feedback loop would occur if both of the first two options held simultaneously, which is disallowed.

The most complex part of the previous section's algorithm is the mutual recursion between the $d=x$ signal and the $c=w$ signal. But since we know that doesn't exist, under the minor assumption that we can generate some ``default value'' for those wires (which is trivially true if modelling them with booleans), we can compute the outputs as follows:

Firstly, suppose that the first of the three options above holds. Generate some default value for $d$. We already have an input value for $a$. Then we can apply the isomorphism and functional intrepretation of $Q$ as described in the previous section to get values for $b$ and $c$. Of course, the output at $b$ may be incorrect, as the input at $d$ was not necessarily correct. However, by assumption, $c$ does not depend on $d$, so the output at $c$ will be correct. Since $c=w$, we now have both inputs for $R$, and both of these are correct. Run $R$ to get the correct value of $x$, and since $x=d$ we now have the correct value of $d$. Run $Q$ again but this time on the correct value of $d$ to get the correct value of $c$.

In short, if $w$ depends on $x$ then we can generate a dummy value for $d$ and run $Q$, $R$ and $Q$ again in that order to get all the correct outputs.

If the second case holds then generate a dummy value for $w$ and run $R$, $Q$, $R$ to get the correct values.

If the third case holds, both of the above will work.

Therefore it follows that in all three cases, we can
\begin{enumerate}
    \item Generate a dummy value for $d$.
    \item Run $Q$ on $a$ and $d$ to get $c$.
    \item Run $R$ on $z$ and that $c$ to get a new $d$.
    \item Run $Q$ on $a$ and the new $d$ to get a new $c$ and a $b$.
    \item Run $R$ on $z$ and the new $c$ to get a $y$.
    \item The output is $b$ and $y$ as obtained in the previous two steps.
\end{enumerate}

But what does it mean to ``run $Q$''? Can we simplify what this means to avoid the isomorphism entirely? The answer is yes, if we design our data structures properly. Rather than splitting our types into two parts, one for inputs and one for outputs, we can use a |DShp| as a parameter to construct a data structure that either contains only the leaves marked as |Left| or only the ones marked as |Right|:

\begin{code}
data Data : Dir -> DShp -> Type where
    LL : type a -> Data Left (L a)
    RR : type a -> Data Right (R a)
    LR : Data Left (R a)
    RL : Data Right (L a)
    Nil : Data d []
    (::) : Data d x -> Data d (T xs) -> Data d (T (x :: xs))
\end{code}

Therefore a |Data Left d| can only contain values at the leaves marked |Left|, and likewise for |Right|. Remember that |type : Typ -> Type| converts from our universe of types to native Idris types. |Typ| represents not only the usable types within our |Ruby| programs, but it also implements a function |rep| that gives our much-needed default values. Together with |gen|, we can generate |Data| for any valid |DShp|:

\begin{code}
rep : (t : Typ) -> type t

gen : {x : DShp} -> {d : Dir} -> Data d x
\end{code}

Unlike with |Rose|, we directly use |Nil| and |(::)| as constructors so we can pattern match with list notation. The reason why we could not do this with |Rose| also is because of the |(::)| constructor of |Data|: it requires a single element |Data d x| and a list |Data d (T xs)|. If we remove the |T| constructor and just have |Nil| and |(::)| then we can't write |Data d (T xs)|, which means we can't enforce that the second argument isn't a leaf.

An interpretation is then a function from the |Rights| on the domain and |Lefts| on the codomain, to the |Lefts| on the domain and the |Rights| on the codomain:

\begin{code}
data Interp : DShp' -> Type where
    Inter : {x, y : DShp}
         -> ((Data Right x, Data Left y) -> (Data Left x, Data Right y))
         -> Interp (x, y)
\end{code}

Writing a handler that compiles a |Ruby| program into an |Interp| is now child's play: given a couple of helper functions

\begin{spec}
swp : Dir -> Dir
swp Left = Right
swp Right = Left

swap : {d : Dir} -> Comp z w => Data d z -> Data (swp d) w
\end{spec}

the simulator handler is given by

\iffalse
\begin{code}
swp : Dir -> Dir
swp Left = Right
swp Right = Left

empty : {d : Dir} -> (if d == Left then Rights x else Lefts x) => Data d x
empty {d=Right} = emptyL where
    emptyL : Lefts y => Data Right y
    emptyL @{VLefts} = RL
    emptyL @{TLefts VLefts'} = []
    emptyL @{TLefts (TLefts' t ts)} with (emptyL @{t})
        _ | v with (emptyL @{TLefts ts})
            _ | vs = v :: vs
empty {d=Left} = emptyR where
    emptyR : Rights y => Data Left y
    emptyR @{VRights} = LR
    emptyR @{TRights VRights'} = []
    emptyR @{TRights (TRights' t ts)} with (emptyR @{t})
        _ | v with (emptyR @{TRights ts})
            _ | vs = v :: vs

swap : {d : Dir} -> Comp z w => Data d z -> Data (swp d) w
swap {d=Left} p = swapL p where
    swapL : Comp x y => Data Left x -> Data Right y
    swapL @{VCompLR} (LL x) = RR x
    swapL @{VCompRL} LR = RL
    swapL @{TComp VComp'} _ = []
    swapL @{TComp (TComp' t ts)} (x :: xs) with (swapL @{t} x)
        _ | v with (swapL @{TComp ts} xs)
            _ | vs = v :: vs
swap {d=Right} p = swapR p where
    swapR : Comp x y => Data Right x -> Data Left y
    swapR @{VCompRL} (RR x) = LL x
    swapR @{VCompLR} RL = LR
    swapR @{TComp VComp'} _ = []
    swapR @{TComp (TComp' t ts)} (x :: xs) with (swapR @{t} x)
        _ | v with (swapR @{TComp ts} xs)
            _ | vs = v :: vs

copy : {d : Dir} -> Comp x y => Data d x
    -> (if d == Left then Lefts x else Rights x)
    => Data (swp d) y
copy {d} p = swap {d=d} p
\end{code}
\fi

\iffalse
\begin{code}
namespace Simulate
\end{code}
\fi

\begin{code}
    gen : Interp x -> Interp x
    gen = id

    algSeq : Interp (a, b) -> Interp (b, c) -> Interp (a, c)
    algSeq (Inter {y=b} f) (Inter g) = Inter $ \(x, y) =>
        let     (_, b1)     = f (x, gen)
                (b2, _)     = g (b1, y)
                (a, b3)     = f (x, b2) 
                (b4, c)     = g (b3, y) in (a, c)

    algPar : Interp (a, b) -> Interp (c, d) -> Interp ([a, c], [b, d])
    algPar (Inter f) (Inter g) = Inter $ \([x, y], [u, v]) =>
        let     (x', u')    = f (x, u)
                (y', v')    = g (y, v) in ([x', y'], [u', v'])

    algInv : {a', b' : DShp} -> Comp a a' => Comp b b' => Interp (a, b) -> Interp (b', a')
    algInv (Inter f) = Inter $ \(y, x) => let (x', y') = f (swap x, swap y) in (swap y', swap x')

    alg : DComb Interp x -> Interp x
    alg (Seq q r) = algSeq q r
    alg (Par q r) = algPar q r
    alg (Inv q) = algInv q

    public export
    RInterp : DShp' -> Type
    RInterp = IFree DComb Interp

    simulate : RInterp x -> Interp x
    simulate = ifold gen alg
\end{code}

where |ifold| is the equivalent of |fold| for |IFree|. Note in particular that the implementation of |algSeq| is precisely the procedure described earlier, where |gen| is used to generate the initial default value for $d$.

There are two key factors in this program: firstly, |simulate| takes a program written in a DSL, and returns a native, first-class Idris function. This is not a symbolic simulation - we have, from within Idris, compiled a program into an Idris function.

The second thing is that this is a function, not a relation. Because we have guaranteed that each primitive is causal, and our DSL guarantees by construction that composition preserves causality, our resulting program is provably causal, and we get out a function at the other end. This also guarantees that the program is implementable in hardware.

We saw earlier how horrific it was to implement |pi_1| in the previous system; does this method fare any better? Judge for yourself:

\iffalse
\begin{code}
namespace Directed
\end{code}
\fi

\begin{code}
    pi_1 : {x, y : DShp} -> Rights y => RInterp ([x, y], x)
    pi_1 = Ret . Inter $ \([a, b], c) => ([c, empty], a)
\end{code}

The implementation is a one-liner, with |empty| simply being a utility function much like swap. For completeness' sake, here are the definitions for |rsh| and |fork| also, again using another small utility function |copy|:

\begin{code}
    rsh : {x, y, z : DShp} -> RInterp ([x, [y, z]], [[x, y], z])
    rsh = Ret . Inter $ \([a, [b, c]], [[d, e], f]) => ([d, [e, f]], [[a, b], c])

    fork : {x, y, z : DShp} -> Fork x y z => RInterp (x, [y, z])
    fork @{pf} = Ret . Inter $ \(a, [b, c]) => case pf of
        Fork1 _     => (empty, [a, a])
        Fork2 _ _   => (b, [empty, copy b])
        Fork3 _ _   => (c, [copy c, empty])
\end{code}


\section{Non-standard interpretation}
\label{sec:nsi}

We have already shown how to write a handler that interprets a Ruby program into an Idris function. In this final section, we will show many more examples of handlers to transform Ruby programs into other forms.


\subsection{Type inference}

We currently have three Ruby DSLs:

\begin{enumerate}
    \item Untyped.
    \item Typed with tuple shapes and data types (typed).
    \item Typed with tuple shapes, data types and directions (directed).
\end{enumerate}

Untyped is a little too relaxed so isn't that useful. Typed encapsulates well-typed relational programs, and directed encapsulates well-typed functional programs. It would therefore be useful to be able to convert typed to directed, if possible. The process for this type inference will be as follows:

\begin{enumerate}
    \item Collect up all the types and constraints of the program.
    \item Solve the system of constraints (if possible).
    \item Use the solution to upgrade the program from the old DSL to the new one.
\end{enumerate}

Unsurprisingly, we will be utilising algebraic effects for two of these steps.


\subsubsection{State, Reader and Error effects}

During the type inference we will need to traverse the syntax tree of our Ruby program, generating variables and storing them and their associated constraints somewhere. Along the way, a malformed program (or an error in the inference algorithm) could cause the program to fail. Therefore we will be making use of the |State|, |Reader| and |Error| effects.

The |State| effect will exactly mimic the |State| monad - it represents a storage box that can be read from and written to. It has two operations, |Get| and |Put|, where |Get| gets the current value stored and |Put| overwrites the stored value with a new value. We therefore define |State| as an effect paramaterised by the type of the internal state:

\hspace{-0.3cm}\begin{minipage}[t]{0.45\textwidth}
\begin{code}
data State s k
    = Get (s -> k)
    | Put s k
\end{code}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{code}
Functor (State s) where
    map f (Get g) = Get (f . g)
    map f (Put s k) = Put s (f k)
\end{code}
\end{minipage}

%format -< = "\subseteq"

In the introduction to algebraic effects, we used |Inl| and |Inr| to work with coproducts. However, there is a more general way of working with arbitrary coproducts: we can define an interface |-<| where |f -< g| witnesses the fact that |g| is a coproduct that contains |f| somewhere inside it. This interface comes with a function |ins| that lifts |f| into |g|, automatically applying |Inl| and |Inr| in the correct places \cite{SwierstraDTC08}.

\begin{code}
infixr 1 -<

interface (Functor f, Functor g) => (-<) f g where
    inj : f a -> g a

    ins : f (Free g a) -> Free g a
    ins = Op . inj

Functor f => (-<) f f where
    inj = id

(Functor f, Functor g) => (-<) f (f :+: g) where
    inj = Inl

(Functor f, Functor g, Functor h, f -< h) => (-<) f (g :+: h) where
    inj = Inr . inj
\end{code}

This assumes that we only ever nest functor coproduct to the right, i.e. |(f :+: g) :+: h| is disallowed. Now that we have this, we can define fully general smart constructors for |Get| and |Put| as follows:

\begin{code}
get : (State s) -< f => Free f s
get = ins (Get pure)

put : (State s) -< f => s -> Free f ()
put s = ins (Put s (pure ()))
\end{code}

Once we handle a |Free (State s) a| we want to give it a starting value for |s| and have it return an |a| as result. Therefore our output type is |s -> a|. There is only one way to write the generator:

%format gen_state
%format alg_state
%format handle_state

\begin{code}
gen_state : a -> (s -> a)
gen_state a = \_ => a
\end{code}

For the algebra, in the |Get| case we continue with the value we've got from the store. In the |Put| case we ignore the value from the store and proceed with the value we've been given:

\begin{code}
alg_state : (State s) (s -> a) -> (s -> a)
alg_state (Get k) = \s => k s s
alg_state (Put s k) = \_ => k s
\end{code}

The handler is then the fold over these two:

\begin{code}
handle_state : Free (State s) a -> (s -> a)
handle_state = fold gen_state alg_state
\end{code}

The implementation for |Reader| is identical except |Get| is replaced by |Read| and there is no |Put|, i.e. it is a read-only state. |Error| is even more straightforward:

\iffalse
\begin{code}
data Reader r k = Read (r -> k)

Functor (Reader r) where
    map f (Read k) = Read (f . k)

read : (Reader r) -< f => Free f r
read = ins (Read pure)

gen_reader : a -> (r -> a)
gen_reader = const

alg_reader : (Reader r) (r -> a) -> (r -> a)
alg_reader (Read k) = \r => k r r

handle_reader : Free (Reader r) a -> (r -> a)
handle_reader = fold gen_reader alg_reader
\end{code}
\fi

%format gen_error
%format alg_error
%format handle_error

\hspace{-0.3cm}\begin{minipage}[t]{0.45\textwidth}
\begin{code}
data Error e k = Throw e

Functor (Error e) where
    map _ (Throw x) = Throw x

throw : (Error e) -< f => e -> Free f b
throw x = ins (Throw x)
\end{code}
\end{minipage}
\vline\hspace{0.3cm}
\begin{minipage}[t]{0.4\textwidth}
\begin{code}
gen_error : a -> Either e a
gen_error = Right

alg_error : (Error e) (Either e a) -> Either e a
alg_error (Throw x) = Left x

handle_error : Free (Error e) a -> Either e a
handle_error = fold gen_error alg_error
\end{code}
\end{minipage}


\subsubsection{The algorithm in detail}

The type inference algorithm traverses the typed program, doing roughly as follows at each step:

\definecolor{dgray}{rgb}{0.2, 0.2, 0.2}
\definecolor{lgray}{rgb}{0.8, 0.8, 0.8}
\newcommand{\dgr}[1]{{\textcolor{dgray}{\textbf{#1}}}}
\newcommand{\lgr}[1]{{\textcolor{lgray}{\textbf{#1}}}}

\begin{enumerate}
    \item \lgr{S}\lgr{E} Get the types of the current term.
    \item \dgr{S}\lgr{E} Generate type variables from the types and store them.
    \item \lgr{S}\lgr{E} Get the constraints of the current term.
    \item \dgr{S}\dgr{E} Generate and store those constraints based on the type variables.
    \item \lgr{S}\lgr{E} Return a program that does the following:
    \begin{enumerate}
        \item \dgr{R}\dgr{E} Look up the type variables in the result store.
        \item \lgr{R}\dgr{E} Check that the resulting types satisfy all the constraints.
        \item \lgr{R}\lgr{E} Return the directed term.
    \end{enumerate}
\end{enumerate}

Beside each type, a dark ``S'' indicases that the |State| effect is being used, and likewise for |Reader| and |Error|. For example, when generating new type variables, the store needs to be read to know which names have already been taken, and then the new variables need to be written. Likewise, when the result types are checked against the constraints, an error may be thrown. (It is at this stage that decidable equality of |Typ| is required, which is the original reason why we constructed our own universe of types rather than simply use |Type|). The SAT solver is run between stages (5) and (a).

It would be a waste of space to regurgitate the entire program here; instead we will look at some example functions that showcase the use of algebraic effects. Firstly some preliminary types:

\iffalse
\begin{code}
Con : Type -> Type
Con = id
\end{code}
\fi

\begin{code}
ST, ET : Type
ST = (List Typ, List (Con Nat))
ET = Unit

Typer1, Typer2 : Type -> Type
Typer1 = Free (State ST :+: Error ET)
Typer2 = Free (Reader (List (Typ, Dir)) :+: Error ET)
\end{code}

|Con| is a datatype that holds information about the various constraints (|Lefts|, |Comp| and so on). Let's examine a couple of the stages from the algorithm outlined above. This function is called during stage (4), putting the constraints into the store:

\begin{code}
putCon : State ST -< f => Con Nat -> Free f Unit
putCon y @{p} = do
    (xs, ys) <- get @{p}
    put @{p} (xs, y :: ys)
\end{code}

Unfortunately, as already explained Idris' type inference is not the best - without explicitly using |xs| and |ys| it can't infer the type of |get| or |put| (it knows the effect is |State s| but can't deduce |s|) so we need to give it the exact type by passing the constraint |State ST -< f| explicitly.

This function is used in step (a), using both |read| from |Reader| and |throw| from |Error|:

\begin{code}
getShp : Rose Nat -> Typer2 DShp
getShp x = do
    ds <- read
    traverse (f ds) x where
    f : List (Typ, Dir) -> Nat -> Typer2 (Typ, Dir)
    f ds n = do
        case inBounds n ds of
            Yes p => pure $ index n ds
            No _ => throw ()
\end{code}

Even though both effects are completely orthogonal, they can be combined with literally zero effort thanks to the coproduct functor and the |-<| typeclass.


\subsubsection{Metaprogramming}

In order to know the current type, target type and necessary constraints, these pieces of data must live somewhere - preferably inside a data structure. Let us take as an example the directed version of |pi_1|:

\begin{spec}
pi_1 : {x, y : DShp} -> Rights y => RInterp ([x, y], x)
\end{spec}

In order to go from a typed |pi_1| to a directed |pi_1|, the previous algorithm needs to know that it:

\begin{itemize}
    \item Takes two implicits, |x| and |y|.
    \item Has one constraint, |Rights y|.
    \item Has result type |([x, y], x)|.
\end{itemize}

In fact, many more pieces of data are needed - for example, the inference algorithm at one point requires a function that deconstructs the result type back into its constituent parts, something like

\begin{spec}
break : DShp' -> Vect 2 DShp
break (T [x, y], _) = [x, y]
break _ = Fail
\end{spec}

In the end, the required data structure containing all this data is rather large, and consists entirely of information that can be derived directly from the signature of |pi_1|.

Luckily, Idris supports metaprogramming via a system known as \emph{elaborator reflection} \cite{ChristiansenElabReflection16}. Using this, we were able to write an elaborator script (i.e. a metaprogram) that takes as input the name of a function and, assuming it is the signature for a Ruby primitive, automatically generate the required data structure needed to upgrade from the typed DSL to the directed one, which is invoked as follows:

%format ` = "\ ''"

\begin{spec}
%runElab makeBlock `{pi_1}
\end{spec}


\subsection{Compiling to RBS}

[The code for this section is 99\% complete, I just need to clean it up and actually write this up properly.]

RBS is the input format used by the Rebecca simulator, a small hardware simulator written in Lazy ML. The format is a list of blocks with each wire of the domain and codomain numbered. Wires with the same number are connected together.

Given a few helper fuctions (|label|, |relabel| and |construct|) and a type |RBS| to represent the lists of blocks and their types, the full interpretation is rather simple to construct. Firstly the generator gets the type and name of the block and constructs an |RBS| for it:

%Doesn't work :(
%format "b.type" = "\Varid{b}\Varid{type}"

\begin{spec}
gen : TBlock x -> Free (State Nat) RBS
gen b = do
    p <- label (fst b.type)
    q <- label (snd b.type)
    pure ((p, q), [(b.name, (p, q))])
\end{spec}

Then the algebra combines and/or rearranges the subterms of each constructor, renaming signals when two types are supposed to match up:

\begin{spec}
alg : TComb (Const (Free (State Nat) RBS) TShp') x -> Free (State Nat) RBS
alg (Seq q r) = do
    ((q1, q2), qs) <- q
    ((r1, r2), rs) <- r
    let f = construct r1 q2 id
    pure ((map f q1, map f r2), relabel f (qs ++ rs))
alg (Par q r) = do
    ((q1, q2), qs) <- q
    ((r1, r2), rs) <- r
    pure (([q1, r1], [q2, r2]), qs ++ rs)
alg (Inv q) = do
    ((q1, q2), qs) <- q
    pure ((q2, q1), qs)
\end{spec}

A simple custom |Show| instance for |RBS| yields the following output for an instance of the program |pi_1 <:> pi_1|:

\noindent
\begin{alltt}
Name      Domain                        Range
----------------------------------------------------------------------
pi1       <<.0, .1>, .2>                <.3, .4>
pi1       <.3, .4>                      .7
----------------------------------------------------------------------
Directions - <<out, in>, in> ~ in
Wiring     - <<.0, .1>, .2> ~ .7
\end{alltt}

Since this algorithm works on typed Ruby, not directed Ruby, the directions for the signals need to be provided separately. However, if the program is a valid causal program, this can be automated by running the type inference algorithm in the previous section to get a valid assignment of directions.

\subsection{Generating circuit diagrams}

[The code for this is also done, I just need to port it from Haskell and to my new system.]


\subsection{Compiling to Verilog}

[I think this would be a good idea, but I haven't started it yet.]


\subsection{Source-to-source transformations}

[I did this in my Master's project so it shouldn't be difficult to port it over.]


\section{Conclusion \& Future Work}
\label{sec:end}

Ruby is a minimal yet expressive language for describing hardware at a very high level. By implementing it as a sequence of DSLs within a functional language, we were able to very easily define the syntax and provide the semantics without writing all the necessary compiler infrastructure ourselves. Dependent typing makes it easy to embed program semantics, such as types, into the syntax; by expressing these types as Idris types, we allowed the host compiler to be our type checker and type inference engine, for the most part.

While algebraic effects were by no means necessary, using them gave us a common and easily implemented recursion scheme over all the different DSLs. We showed how non-standard interpretations are given as handlers; once the language and typing rules were defined, writing these interpretations was straightforward. Interpretations to Idris functions, RBS, and circuit diagrams were all written fairly quickly and easily. The running example of longest path estimation showed how to use handlers to extract valuable low-level properties from a high-level design, while at a higher level we have shown how handlers can be used to perform source-to-source transformation.

The translation from typed to directed Ruby illustrated how easy metaprogramming is when using algebraic effects: the type inference algorithm uses a typed Ruby program to generate a stateful error-throwing program, which generates a read-only-stateful error-throwing program, which generates a directed Ruby program, which generates an Idris function. The ability to compose effects with zero extra effort made writing the algorithm relatively smooth in comparison to using an alternative such as monad transformers, and we expect that this approach of chaining program generators together has many more uses to be discovered.

Unfortunately, not everything proceeded smoothly. The very non-standard typing rules for Ruby meant that directed programs had to be broken into smaller chunks lest the compiler fail to satisfy all the necessary constraints. This had the knock-on effect that when upgrading from typed Ruby to directed Ruby, run-time polymorphism is lost. Writing functional interpretations for causal relations is also slightly clunkier than in the original Lazy ML version. While Idris did offer a plethora of features to make our lives easier - full dependent types, determining arguments for data structures, automatic proof search, proof hints, elaborator reflection - this was not enough to be able to write directed Ruby as easily as we would have liked. Still, this project is, as far as we know, the first implementation of Ruby that supports correct-by-construction programming and the notion of causal programs, allowing for suitable programs to be simulated functionally rather than relationally.

Here are some further avenues of research that seem promising and interesting:
\begin{itemize}
    \item Use algebraic effects to combine relational, functional, declarative and imperative languages together, so that the programmer can get the benefit of using whichever paradigm is best suited for the current task.
    \item Explore how to use algebraic effects to chain program-generating programs together. In particular, explore how this could be used to write programs that generate compilers or optimisation engines, for example for design space exploration.
    \item Embed hardware DSLs inside existing synthesis tools, for example a DSL for designing state machines that can be used in the middle of SystemVerilog or VHDL code.
    \item Investigate the spectrum between non-verified code (such as the Lazy ML Ruby compiler) and fully verified code (such as \emph{Coby}, a Coq library for Ruby verification \cite{WangCoby21}), and see where on that spectrum are the most useful tools for practical hardware design.
    \item Explore DSLs for hardware-software co-design, allowing one to simultaneously program CPU, GPU and FPGA - Aronsson and Sheeran's work in \cite{AronssonHardware17} uses a system remarkably close to algebraic effects.
\end{itemize}


%%
%% Acknowledgements
\begin{acks}

\end{acks}

%%
%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

\listoffixmes

\end{document}
\endinput
