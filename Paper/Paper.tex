\documentclass[acmsmall,draft]{acmart}

%%
%% Formatting directives for lhs2TeX
%include polycode.fmt
%format forall(x)   = forall_ x "\hsforall"
%format .           = "\hsdot{" `comp_` "}{" period_ "}"
%format `comp_`     = "\circ "
%format period_     = "\,.\,"
%format forall_     = "\forall "
\makeatletter

\let\HaskellResetHook\empty
\newcommand*{\AtHaskellReset}[1]{%
    \g@@addto@@macro\HaskellResetHook{#1}}
\newcommand*{\HaskellReset}{\HaskellResetHook}

\global\let\hsforallread\empty

\newcommand\hsforall{\global\let\hsdot=\hsperiodonce}
\newcommand*\hsperiodonce[2]{#2\global\let\hsdot=\hscompose}
\newcommand*\hscompose[2]{#1}

\AtHaskellReset{\global\let\hsdot=\hscompose}

\HaskellReset

\makeatother
\EndFmtInput

\iffalse
\begin{code}
import Data.Vect
\end{code}
\fi

\usepackage{fixme}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{ebproof}
\usepackage{cleveref}
\usepackage{quiver}

\DeclareMathOperator{\delay}{delay}
\usetikzlibrary{arrows.meta}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.
\fxwarning{Rights}
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{????}
\acmDOI{????}


%%
%% These commands are for a JOURNAL article.
\fxwarning{Journal info}
\acmJournal{JACM}
\acmVolume{37}
\acmNumber{4}
\acmArticle{111}
\acmMonth{8}

%%
%% Submission ID.
%%\acmSubmissionID{123-A56-BU3}

\begin{document}

\title{Ruby on Roads}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Omar Tahir}
\email{omar.tahir17@@ic.ac.uk}
\orcid{???}
\affiliation{%
  \institution{Imperial College London}
  \city{London}
  \country{UK}
}

\renewcommand{\shortauthors}{Tahir}

\begin{abstract}
    \fxwarning{Abstract}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
    <concept_id>10011007.10011006.10011050.10011017</concept_id>
    <concept_desc>Software and its engineering~Domain specific languages</concept_desc>
    <concept_significance>500</concept_significance>
</concept>
<concept>
    <concept_id>10010583.10010600.10010628</concept_id>
    <concept_desc>Hardware~Reconfigurable logic and FPGAs</concept_desc>
    <concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Domain specific languages}
\ccsdesc[500]{Hardware~Reconfigurable logic and FPGAs}

\keywords{relational programming, functional programming, hardware design}

\maketitle

\section{Introduction}

\fxwarning{Put an intro to the intro}

\subsection{Ruby}

\emph{Ruby} is a simple relational language used for hardware descriptions.

A relation on sets $A$ and $B$ is a subset $R \subseteq A \times B$, written $R : A \leftrightarrow B$. If $(a,b) \in R$ then we write $a R b$, which should be read as ``$a$ is related to $b$ by $R$''. Most properties and relationships between data can be represented by relations. In particular, every function is a relation: if $f : A \to B$ then the relation corresponding to $f$ is the set $\{(x, f(x)) \mid x \in A\} \subseteq A \times B$, which we usually also call $f$. The space of relations is strictly larger than the space of functions: for example, the relation \[
    \{(T,T),(T,F),(F,T),(F,F)\} : \mathsf{Bool} \leftrightarrow \mathsf{Bool}
\] cannot be represented as a function $f : \mathsf{Bool} \to \mathsf{Bool}$, because both $T$ and $F$ are sent to both $T$ and $F$. In fact, this particular relation is the same as $\mathsf{Bool} \times \mathsf{Bool}$ and is called the full relation on $\mathsf{Bool}$.

It is very easy to draw pictures of relations. Below are pictures of two relations $R : X \leftrightarrow Y$ and $S : A \times B \leftrightarrow C \times D$. We call the left side of the relation the domain, and the right side the codomain or range. When we draw relations that have a pair at both the domain and codomain, we can choose to draw the relation as four-sided; this is useful for circuit design. Note that we read values from the bottom upwards, and that in the four-sided case the domain is $\left<left,top\right>$ and the codomain is $\left<bottom,right\right>$.

\begin{figure}[htpb]
    \centering
    % R
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \draw (1, 0) rectangle (3, 2);
            \node at (2, 1) (R) {$R$};
            \node at (0, 1) (x) {$x$};
            \node at (4, 1) (y) {$y$};
            \draw (x) -- (1, 1);
            \draw (3, 1) -- (y);
        \end{tikzpicture}
    \end{subfigure}
    % S (2-sided)
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \node at (2, 1) (S) {$S$};
            \node at (0, 0.65) (a) {$a$};
            \node at (0, 1.35) (b) {$b$};
            \node at (4, 0.65) (c) {$c$};
            \node at (4, 1.35) (d) {$d$};
            \draw (1, 0) rectangle (3, 2);
            \draw (a) -- (1, 0.65);
            \draw (b) -- (1, 1.35);
            \draw (c) -- (3, 0.65);
            \draw (d) -- (3, 1.35);
        \end{tikzpicture}
    \end{subfigure}
    % S (4-sided)
    \begin{subfigure}[c]{0.3\textwidth}
        \begin{tikzpicture}[scale=0.9]
            \node at (2, 2) (S) {$S$};
            \node at (0, 2) (a') {$a$};
            \node at (2, 4) (b') {$b$};
            \node at (2, 0) (c') {$c$};
            \node at (4, 2) (d') {$d$};
            \draw (1, 1) rectangle (3, 3);
            \draw (a') -- (1, 2);
            \draw (b') -- (2, 3);
            \draw (c') -- (2, 1);
            \draw (d') -- (3, 2);
        \end{tikzpicture}
    \end{subfigure}
    \caption{A drawing of $R : X \leftrightarrow Y$ and two different drawings of $S : A \times B \leftrightarrow C \times D$.}
    \label{fig:relation}
\end{figure}

As it turns out, drawing pictures for relations is not just a visual exercise: there is a formal way of converting between the syntax of \fxwarning*{allegories}{relational programs and pictures}.

There are three foundational operators defined on relations, which form the basic combinators for the language \emph{Ruby}:

\begin{align*}
    (-;-) &: (A \leftrightarrow B) \to (B \leftrightarrow C) \to (A \leftrightarrow C), \\
    [-,-] &: (A \leftrightarrow B) \to (C \leftrightarrow D) \to (A \times C \leftrightarrow B \times D), \\
    (-)^{-1} &: (A \leftrightarrow B) \to (B \leftrightarrow A)
.\end{align*}

The first $(-;-)$ is for sequential composition and has the following definition: \[
    x(R;S)y \iff \exists p\ :\ xRp \wedge pSy
.\] This is the most tricky of the three combinators to actually implement, due to the fact that the value of $z$ must often be inferred; we will revisit this issue later. It can be drawn as follows:
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture} 
        \node at (-0.5, 1) (x) {$x$};
        \node at (2, 1) (R) {$R$};
        \node at (4, 1.2) (p) {$p$};
        \node at (6, 1) (S) {$S$};
        \node at (8.5, 1) (y) {$y$};

        \draw (x) -- (1, 1);
        \draw (1, 0) rectangle (3, 2);
        \draw (3, 1) -- (5, 1);
        \draw (5, 0) rectangle (7, 2);
        \draw (7, 1) -- (y);

        \node at (4, 2.2) (RS) {$R;S$};
        \draw[dashed] (0.5, -0.5) rectangle (7.5, 2.5);
    \end{tikzpicture}
    \caption{Sequential composition of relations.}
    \label{fig:rel_seq}
\end{figure}

The second is for parallel composition and is defined by \[
    (x,a)[R,S](y,b) \iff xRy \wedge aSb
.\]
\begin{figure}[htpb]
    \centering
    % Par
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{tikzpicture}
            \node at (-0.5, 1) (x) {$x$};
            \node at (2, 1) (R) {$R$};
            \node at (4.5, 1) (y) {$y$};
    
            \node at (-0.5, 3.5) (a) {$a$};
            \node at (2, 3.5) (S) {$S$};
            \node at (4.5, 3.5) (b) {$b$};
            
            \draw (x) -- (1, 1);
            \draw (1, 0) rectangle (3, 2);
            \draw (3, 1) -- (y);
    
            \draw (a) -- (1, 3.5);
            \draw (1, 2.5) rectangle (3, 4.5);
            \draw (3, 3.5) -- (b);
    
            \node at (2, 5) (RS) {$[R,S]$};
            \draw[dashed] (0.5, -0.5) rectangle (3.5, 5.5);
        \end{tikzpicture}
    \end{subfigure}
    % Inv
    \begin{subfigure}[c]{0.45\textwidth}
        % Inv (bend)
        \begin{subfigure}{0.9\textwidth}
            \begin{tikzpicture}
                \draw (1, 0) rectangle (3, 2);
                \node at (2, 1) (R) {$R^{-1}$};
                \node at (-0.2, 1) (x) {$x$};
                \node at (4.5, 1) (y) {$y$};

                \draw (x) -- (0.3, 1) -- (0.3, 2.5) -- (3.3, 2.5) -- (3.3, 1) -- (3, 1);
                \draw (y) -- (3.7, 1) -- (3.7, -0.5) -- (0.7, -0.5) -- (0.7, 1) -- (1, 1);
            \end{tikzpicture}
        \end{subfigure}
        
        \bigbreak
        \bigbreak
        
        % Inv (no bend)
        \begin{subfigure}{0.9\textwidth}
            \begin{tikzpicture}
                \draw (1, 0) rectangle (3, 2);
                \node at (2, 1) (R) {$R^{-1}$};
                \node at (0, 1) (y) {$y$};
                \node at (4, 1) (x) {$x$};
                \draw (1, 1) -- (y);
                \draw (x) -- (3, 1);
            \end{tikzpicture}
        \end{subfigure}
    \end{subfigure}

    \caption{Parallel composition of relations, and two ways of drawing the inverse of a relation.}
    \label{fig:rel_par_inv}
\end{figure}

The third is called the inverse or converse, and is defined by \[
    xR^{-1}y \iff yRx
.\] Unlike functions, all relations are invertible. You can draw them in two ways - by bending the wires or just starting them from the opposite sides to usual. Along with these combinators, there are a group of very useful polymorphic relations that we call ``rewiring'' relations. Here are the five most useful ones:
\begin{align*}
    id &: A \leftrightarrow A & x\ &id\ y, \\
    fork &: A \leftrightarrow A \times A & x\ &fork\ \left<x,x\right>, \\
    \pi_{1} &: A \times B \leftrightarrow A & \left<x,y\right>\ &\pi_{1}\ x, \\
    \pi_{2} &: A \times B \leftrightarrow B & \left<x,y\right>\ &\pi_{2}\ y, \\
    rsh &: A \times (B \times C) \leftrightarrow (A \times B) \times C & \left<x,\left<y,z\right>\right>\ &lsh\ \left<\left<x,y\right>,z\right>
.\end{align*}

We use angle brackets $\left<-,-\right>$ for products because we are working in the category of tuples over some carrier $C$. We will be implementing these tuples as rose trees.

To combine all these concepts together, here is a small relation that we will be using in the future. We call it $fork2$, and it is defined by
\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{align*}
            &fork2 = \pi_{1}^{-1} ; [fork^{-1}, fork] ; rsh \\
            &\implies \left<x,x\right> fork2 \left< \left< x,y\right>,y\right>
        .\end{align*}        
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \begin{tikzpicture}
            \node at (0, 2) (x1) {$x$};
            \node at (2, 4) (x2) {$x$};
            \node at (1.65, 0) (x3) {$x$};
            \node at (2.35, 0) (y1) {$y$};
            \node at (4, 2) (y2) {$y$};
    
            \node at (2, 2) (fork2) {$fork2$};
            \draw (1, 1) rectangle (3, 3);
            \draw (x1) -- (1, 2);
            \draw (x2) -- (2, 3);
            \draw (1.65, 1) -- (x3);
            \draw (2.35, 1) -- (y1);
            \draw (3, 2) -- (y2);
        \end{tikzpicture}
    \end{subfigure}
    \caption{The definition of $fork2$ and a picture of it.}
    \label{fig:fork2}
\end{figure}


\subsection{Algebraic Effects}

Algebraic effects are a new technique for developing DSLs that is gaining more traction in functional programming. Here we will briefly explain the motivation for algebraic effects and how they are implemented. Our implementation will be in Idris 2, but most of the code in this section is, with minor modifications, valid Haskell.

Suppose we want to develop a DSL for some simple expression language that has two operations, addition and multiplication, and we want this expression language to be polymorphic in the type of vaules it operates on. The simplest way to develop such a DSL would be by defining the type of syntax trees as follows:

%format Expr_1
%format Val_1
%format Add_1
%format Neg_1

\begin{code}
data Expr_1 a = Val_1 a | Add_1 (Expr_1 a) (Expr_1 a) | Neg_1 (Expr_1 a)    
\end{code}

A term in this language looks as you would expect:

%format term_1

\begin{code}
term_1 : Expr_1 Int
term_1 = Add_1 (Neg_1 (Val_1 2)) (Val_1 3)
\end{code}

An evaluation function is needed to interpret this language, and is simple enough to write just by recursing over the structure of the tree:

%format eval_1

\begin{code}
eval_1 : Expr_1 Int -> Int
eval_1 (Val_1 x) = x
eval_1 (Add_1 x y) = eval_1 x + eval_1 y
eval_1 (Neg_1 x) = -(eval_1 x)
\end{code}

Algebraic effects are similar, except that explicit recursion is replaced by implicit recursion over some continuation parameter |k|:

%format Expr_2
%format Add_2
%format Neg_2
%format term_2

\begin{code}
data Expr_2 a k = Val_2 a | Add_2 k k | Neg_2 k

data Fix : (Type -> Type) -> Type where
    In:  f (Fix f) -> Fix f

term_2 : Fix (Expr_2 Int)
term_2 = In (Add_2 (In (Neg_2 (In (Val_2 2)))) (In (Val_2 3)))
\end{code}

The |Fix| datatype takes a signature |f| that has children of parameter |k| and sets |k = Fix f|, which causes recursion. If |f| is a functor, an interpretation is given quite easily through a function known as an algebra:

%format fold_fix
%format alg_2

\begin{code}
Functor (Expr_2 a) where
    map f (Val_2 a) = Val_2 a
    map f (Add_2 x y) = Add_2 (f x) (f y)
    map f (Neg_2 x) = Neg_2 (f x)

alg_2 : Expr_2 Int Int -> Int
alg_2 (Val_2 x) = x
alg_2 (Add_2 x y) = x + y
alg_2 (Neg_2 x) = -x

fold_fix : Functor f => (f a -> a) -> Fix f -> a
fold_fix alg (In x) = alg (map (fold_fix alg) x)
\end{code}

|fold_fix| applies the algebra |alg| recursively through the syntax tree, folding it into a single value. We call such a thing a \emph{handler}:

%format handler_2

\begin{code}
handler_2 : Fix (Expr_2 Int) -> Int
handler_2 = fold_fix alg_2
\end{code}

This seems all quite redundant at the moment - we have turned one datatype into two, we require one of them to be a functor, and the terms themselves are longer as every operation has to be preceded by an |In| constructor. That is because the benefit comes when we want to combine multiple DSLs together: using this system, it is trivial to take the coproduct of two or more signatures, resulting in the ability to use operations from either DSL:

%format :+: = "+"

\begin{code}
infixr 1 :+:
data (:+:) : (f : Type -> Type) -> (g : Type -> Type) -> a -> Type where
    Inl : f a -> (f :+: g) a
    Inr : g a -> (f :+: g) a

(Functor f, Functor g) => Functor (f :+: g) where
    map f (Inl x) = Inl (map f x)
    map f (Inr y) = Inr (map f y)
\end{code}

The coproduct combinator |:+:| is used to combine two signatures together, and at each stage you choose whether you wil use an operation from the left signature with |Inl| or from the right with |Inr|. It's like a recursive |Either|.

For example, suppose we want another operation, say multiplication. Using the original system, we would have to add a |Mul| constructor to the |Expr1| datatype, which would break all our existing code. With this system, we can make a completely new datatype just for |Mul|, and combine it with our existing one. Firstly, we define this new datatype:

\begin{spec}
data Expr' a k : Val' a | Mul k k
\end{spec}

However, note how there is redundancy in |Expr_2| and |Expr'| - both are paramaterised by the same values (|a| and |k|) and both need a |Val| constructor to represent plain values. This is excessive because both |Val| constuctors serve exactly the same purpose. Instead, we can factor out the leaf value constructor into |Fix|, and we call this new datatype |Free|:

\hspace{-0.3cm}\begin{minipage}[t]{0.50\textwidth}
\begin{code}
data Free : (Type -> Type) -> Type -> Type
    where
    Var : a -> Free f a
    Op : f (Free f a) -> Free f a
\end{code}
\begin{code}
Functor f => Functor (Free f) where
    map f (Var x) = Var (f x)
    map f (Op op) = Op (map (map f) op)
\end{code}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
\begin{code}
Functor f => Applicative (Free f) where
    pure = Var
    (Var f) <*> x = map f x
    (Op op) <*> x = Op (map (<*>x) op)
\end{code}
\begin{code}
Functor f => Monad (Free f) where
    (Var x) >>= f = f x
    (Op op) >>= f = Op (map (>>=f) op)
\end{code}
\end{minipage}

\bigbreak

|Free f a| is precisely the type of free monads over the functor |f| with carrier |a|. We can rewrite |Expr_2| to use |Free| as follows:

%format Expr_3
%format Add_3
%format Neg_3
%format term_3

\begin{code}
data Expr_3 k = Add_3 k k | Neg_3 k

Functor Expr_3  where
    map f (Add_3 x y) = Add_3 (f x) (f y)
    map f (Neg_3 x) = Neg_3 (f x)

term_3 : Free Expr_3 Int
term_3 = Op (Add_3 (Op (Neg_3 (Var 2))) (Var 3))
\end{code}

Folding a |Free| requires an algebra for the recursive case, and a generator for the leaf case:

%format gen_3
%format alg_3

\begin{code}
gen_3 : Int -> Int
gen_3 = id

alg_3 : Expr_3 Int -> Int
alg_3 (Add_3 x y) = x + y
alg_3 (Neg_3 x) = -x
\end{code}

Folding is done almost identically to |Fix|, we just have to handle the |Var| case with the generator:

%format \_ = "\lambda \anonymous"
%format handler_3

\begin{code}
fold : Functor f => (a -> b) -> (f b -> b) -> (Free f a -> b)
fold gen _ (Var x) = gen x
fold gen alg (Op op) = alg (map (fold gen alg) op)
    
handler_3 : Free Expr_3 Int -> Int
handler_3 = fold gen_3 alg_3
\end{code}

The new datatype with |Mul| no longer needs a |Val'| constructor as it's been absorbed into |Free|, and the algebra is simple:

%format Expr_4
%format alg_4

\begin{code}
data Expr_4 k = Mul k k

Functor Expr_4 where
    map f (Mul x y) = Mul (f x) (f y)

alg_4 : Expr_4 Int -> Int
alg_4 (Mul x y) = x * y
\end{code}

%format term_34

For example, a term using both languages would look like

\begin{code}
term_34 : Free (Expr_3 :+: Expr_4) Int
term_34 = Op (Inl (Add_3 (Var 3) (Op (Inr (Mul (Var 2) (Op (Inl (Neg_3 (Var 4)))))))))
\end{code}

Notice how we go left for |Add| and |Neg|, right for |Mul|, and the |Var| case is part of |Free| so doesn't need to go either left or right.

Since both languages will share the same carrier |a|, the generator will also be shared between them, but the algebras must be combined to work on their coproduct.

%format </> = "\nabla"
%format alg_f
%format alg_g

\begin{code}
infixr 1 </>
(</>) : (f a -> a) -> (g a -> a) -> (f :+: g) a -> a
(alg_f </> _) (Inl x) = alg_f x
(_ </> alg_g) (Inr y) = alg_g y
\end{code}

The final handler that can handle both languages simultaneously is then

%format handler_34

\begin{code}
handler_34 : Free (Expr_3 :+: Expr_4) Int -> Int
handler_34 = fold gen_3 (alg_3 </> alg_4)
\end{code}


\section{Ruby as an embedded DSL}

In this section we will explore the various ways of embedding Ruby into Idris. The goal is to eventually have a DSL that represents exactly the valid Ruby programs. What ``valid'' actually means is something we will refine as we go, which means we will be producing a sequence of DSLs, each more restrictive than the last, and translators that can go from one step to the next (if the program is valid).

\subsection{Untyped Ruby}

The most basic embedding of Ruby is as a completely untyped language. This means that as long as the terms are syntactically valid, they typecheck. In other words, this DSL has no built-in semantics: it is precisely the words of the language with operators $(-;-)$, $[-,-]$ and $(-)^{-1}$.

%format gen_delay
%format alg_delay

\begin{code}
data RComb k = Seq k k | Par k k | Inv k

Functor RComb where
    map f (Seq x y) = Seq (f x) (f y)
    map f (Par x y) = Par (f x) (f y)
    map f (Inv x) = Inv (f x)

Ruby : Type -> Type
Ruby = Free RComb
\end{code}

For example, our $fork2$ program could be written as

%format pi_1

\begin{code}
fork2 : Ruby String
fork2 = Op (Seq (Op (Inv (Var "pi1")))
           (Op (Seq (Op (Par (Op (Inv (Var "fork"))) (Var "fork")))
               (Var "rsh"))))
\end{code}

Writing a handler for this DSL is straightforward. For example, suppose we would like to apply nonstandard analysis to calculate the length of the longest path through the circuit. Intuitively, the semantics would be given by the equations

\begin{align*}
    \delay(Q;R) &= \delay(Q) + \delay(R) \\
    \delay([Q,R]) &= \max(\delay(Q),\delay(R)) \\
    \delay(Q^{-1}) &= \delay(Q)
.\end{align*}

Assuming that all primitive blocks have a delay of one cycle (a terrible assumption but sufficient for this example), the analysis would be given as a handler as follows:

\begin{code}
analyseDelay : Ruby a -> Nat
analyseDelay = fold gen_delay alg_delay where
    gen_delay : a -> Nat
    gen_delay = const 1

    alg_delay : RComb Nat -> Nat
    alg_delay (Seq x y) = x + y
    alg_delay (Par x y) = max x y
    alg_delay (Inv x) = x
\end{code}

We could, of course, change the type of the program to something like |Ruby (String, Nat)| where the second element of the pair is the delay. The generator would then be |gen_delay = snd|. Or we could store a lookup table of primitives and information about them, such as delay, and pass it to the generator to use during the fold.

\iffalse
\begin{code}
Show (RComb String) where
    show (Seq x y) = x ++ " ; " ++ y
    show (Par x y) = "[" ++ x ++ ", " ++ y ++ "]"
    show (Inv x) = "inv " ++ x

[freeStr] Functor f => Show (f String) => Show (Free f String) where
    show (Var x) = x
    show (Op x) = show (map show x)

Show a => Functor f => Show (f String) => Show (Free f a) where
    show (Var x) = show x
    show (Op x) = show (map show x)
\end{code}
\fi

\subsection{Typed Ruby, by tuple shape}

Now we would like to start making sure Ruby programs make some semantic sense. The first thing to check is that wires are connected together properly. Let $a,b,\ldots$ range over tuples of wires denoted by angled brackets, and let $W$ denote a single wire. For example, the tuple $\left<a,W\right>$ is a pair whose first element is any tuple, and whose second element is a single wire. We therefore have the following typing rules:

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \hypo{R : b \leftrightarrow c}
            \infer2[Seq]{(Q;R) : a \leftrightarrow c}
        \end{prooftree}
    \end{subfigure}
    \begin{subfigure}[c]{0.45\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \hypo{R : c \leftrightarrow d}
            \infer2[Par]{[Q,R] : \left<a, c\right> \leftrightarrow \left<b, d\right>}
        \end{prooftree}
    \end{subfigure}

    \bigbreak

    \begin{subfigure}[c]{\textwidth}
        \centering
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \infer1[Inv]{Q^{-1} : b \leftrightarrow a}
        \end{prooftree}
    \end{subfigure}
    \caption{Typing rules for Seq, Par and Inv.}
    \label{fig:typing1}
\end{figure}

This prevents a block that has two wires on the codomain from connecting to a block with three wires in its domain, for example.

Looking back at \cref{fig:fork2}, we can see that the type of |fork2| is two signals on the domain and three on the codomain. The signals are completely polymorphic (i.e. $x$ and $y$ are independent and could be any number of wires in any arrangement). A derivation of the type of |fork2| follows:

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{\pi_1 &: \left<\left<a,a\right>,a\right> \leftrightarrow \left<a,a\right>}
        \infer1[Inv]{\pi_1^{-1} &: \left<a,a\right> \leftrightarrow \left<\left<a,a\right>,a\right>}
        \hypo{fork &: a \leftrightarrow \left<a,a\right>}
        \infer1[Inv]{fork^{-1} &: \left<a,a\right> \leftrightarrow a}
        \hypo{fork : b \leftrightarrow \left<b,b\right>}
        \infer2[Par]{[fork^{-1}, fork] : \left<\left<a,a\right>,b\right> \leftrightarrow \left<a,\left<b,b\right>\right>}
        \infer2[Seq]{\pi_1^{-1}\ ;\ [fork^{-1}, fork] : \left<a,a\right> &\leftrightarrow \left<a,\left<b,b\right>\right>}
        \hypo{\hspace{-4.5cm}rsh : \left<a,\left<b,b\right>\right> \leftrightarrow \left<\left<a,b\right>,b\right>}
        \infer2[Seq]{\pi_1^{-1}\ ;\ [fork^{-1}, fork]\ ;\ rsh : \left<a,a\right> \leftrightarrow \left<\left<a,b\right>,b\right>}
    \end{prooftree}
    \caption{A derivation of $fork2 : \left<a,a\right> \leftrightarrow \left<\left<a,b\right>,b\right>$.}
    \label{fig:fork2deriv}
\end{figure}

Here we have implicitly used the polymorphism of $fork$, $rsh$ and $\pi_{1}$ to instantiate them to the correct types; we will revisit polymorphism later and make the typing rules for it explicit. For now, since we will be embedding this into Idris' type system, implicit polymorphism in derivations is fine.

Now we need a way to represent these types. Since they are tuples of arbitrary size and depth, we will be using rose trees:

\begin{code}
data Rose a = V a | T (Vect n (Rose a))

Functor Rose where
    map f (V x) = V (f x)
    map f (T xs) = T (map (map f) xs)
\end{code}

We choose to use the length-indexed |Vect| type over the non-indexed |List| type because knowing the length statically will help us later when constructing various objects that rely on it.

For ease of use, we also define the following aliases:

\begin{code}
Shp : Type
Shp = Rose Unit

W : Shp
W = V ()
\end{code}

The types of relations are then of the form |(Shp, Shp)|. It is also convenient to write |W| rather than |V ()| everywhere. For now, we will only need rose trees of type |Unit|, however in future we will need to paramaterise by other types.

\iffalse
\begin{code}
vectEq : Eq a => (xs : Vect n a) -> (ys : Vect m a) -> Bool
vectEq (x::xs) (y::ys) = x == y && vectEq xs ys
vectEq [] [] = True
vectEq _ _ = False

Eq a => Eq (Rose a) where
    (V x) == (V y) = x == y
    (T xs) == (T ys) = vectEq xs ys
    _ == _ = False
\end{code}
\fi

\subsubsection{Smart constructors}

One way of enforcing that these typing rules are adhered to is to use so-called ``smart'' constructors - hide the real data constructors and expose an interface of constructors that only allow for the construction of well-typed terms. For example, here is the definition of the smart constructor that replaces |Seq| (as a bonus, we can define it as a custom infix operator too):

%format <:> = "\,;"

\iffalse
\begin{code}
namespace Smart
\end{code}
\fi

\begin{code}
    Ruby' : Type -> Type
    Ruby' a = Maybe (Ruby a, (Shp, Shp))

    infixl 1 <:>
    (<:>) : Ruby' a -> Ruby' a -> Ruby' a
    (q <:> r) = do
        (q', (ql, qr)) <- q
        (r', (rl, rr)) <- r
        if qr == rl then Just (Op (Seq q' r'), (ql, rr)) else Nothing
\end{code}

First, note the type that we are using: we need to pack a |Ruby a| (that represents the actual program) together with its type. Since the constructor can fail if the types don't match, we wrap the whole thing in a |Maybe|.

Note also that we can't store the types inside the program itself - |Ruby (a, (Shp, Shp))| won't work. This is because there is no way to access those values without interpreting the program, whereas here we are trying to \emph{construct} the program only.

\iffalse
\begin{code}
    infixl 1 <|>
    (<|>) : Ruby' a -> Ruby' a -> Ruby' a
    (q <|> r) = do
        (q', (ql, qr)) <- q
        (r', (rl, rr)) <- r
        Just (Op (Par q' r'), (T [ql, rl], T [qr, rr]))

    inv : Ruby' a -> Ruby' a
    inv q = do
        (q', (ql, qr)) <- q
        Just (Op (Inv q'), (qr, ql))
\end{code}
\fi

|Par| is also replaced by the pipe operator @|@ and |Inv| with the function |inv|. Together, we can now write our |fork2| anew:

%format <|> = "\,\vert\,"

\begin{code}
    build : a -> Shp -> Shp -> Ruby' a
    build n x y = Just (pure n, (x, y))

    pi_1 : Ruby' String
    pi_1 = build "pi1" (T [T [W, W], W]) (T [W, W])

    fork : Ruby' String
    fork = build "fork" W (T [W, W])

    rsh : Ruby' String
    rsh = build "rsh" (T [W, T [W, W]]) (T [T [W, W], W])

    fork2 : Ruby' String
    fork2 = inv pi_1 <:> (inv fork <|> fork) <:> rsh
\end{code}

You may be wondering, why must we specify the types so strictly? In the derivation in \cref{fig:fork2deriv} we showed that $fork2$ has type $\left<a,a\right> \leftrightarrow \left<\left<a,b\right>,b\right>$, yet here we are specifying it with $a = b = W$. Likewise, the full type of $\pi_{1}$ is $\left<a,b\right> \leftrightarrow a$. Therefore the type of |pi_1| should rightly be

\begin{spec}
pi_1 : {x, y : Shp} -> Ruby' String
pi_1 {x, y} = build "pi1" (T [x, y]) x
\end{spec}

Here the |{x,y}| are optional paramaters that the compiler tries to fill in automatically. And therein lies the problem: the types of these programs cannot be deduced at runtime, but there is not enough information for them to be deduced at compile time either. Therefore we must give each subprogram its exact type, breaking the polymorphism of these blocks, and reducing the usefulness of the DSL. The other option is to use the polymorphic variants of |pi_1|, |fork| and |rsh| but then pass the types explicitly like

\begin{spec}
fork2 : Ruby' String
fork2 = inv (pi_1 {x=T[T[W,W],W],y=T[W,W]}) <:>
    (inv (fork {x=W}) <|> (fork {x=W})) <:>
    (rsh {x=W,y=W,z=W})
\end{spec}

which is completely unreadable. However, with the power of dependent typing, we can move this information to the type level, which will make these values resolvable at compile time.


\subsubsection{Dependent Types}

We can move type information to the type level by introducing a datatype paramaterised by the type of the program:

\iffalse
\begin{code}
namespace Dep
\end{code}
\fi

\begin{code}
    data Typed : Type -> (Shp, Shp) -> Type where
        Tp : k -> Typed k t
\end{code}

We can paramaterise Ruby programs as follows:

\begin{code}
    TRuby : (Shp, Shp) -> Type -> Type
    TRuby t a = Typed (Free RComb a) t
\end{code}

The type of the Ruby program is now embedded at the Idris type level, which means we can allow the Idris type system to do type inference, checking and unification for us:

\begin{code}
    (<:>) : TRuby (a, b) t -> TRuby (b, c) t -> TRuby (a, c) t
    ((Tp q) <:> (Tp r)) = Tp (Op (Seq q r))
\end{code}

Here the |a|, |b| and |c| are implicit arguments like earlier (we could have used the |{a,b,c : Shp}| notation from before but it's not necessary here), however in this case those arguments are embedded within the |Typed| datatype which makes them compile-time values rather than runtime values. Note that unlike the smart constructors in the preceding section, these constructors aren't doing any additional checking - this constructor is just an alias to give us an infix operator and save us from writing |Tp (Op (Seq _ _))| everywhere.

After giving similar definitions for parallel and inverse, we get to write our |fork2| program from before, but with full polymorphism:

\iffalse
\begin{code}
    (<|>) : TRuby (a, b) t -> TRuby (c, d) t -> TRuby (T [a, c], T [b, d]) t
    ((Tp q) <|> (Tp r)) = Tp (Op (Par q r))

    inv : TRuby (a, b) t -> TRuby (b, a) t
    inv (Tp q) = Tp (Op (Inv q))
\end{code}
\fi

\begin{code}
    build : t -> (x : (Shp, Shp)) -> TRuby x t
    build v _ = Tp (Var v)

    fork : {u : Shp} -> TRuby (u, T [u, u]) String
    fork = build "fork" (u, T [u, u])

    pi_1 : {u, v : Shp} -> TRuby (T [u, v], u) String
    pi_1 = build "pi1" (T [u, v], u)
    
    rsh : {u, v, w : Shp} -> TRuby (T [u, T [v, w]], T [T [u, v], w]) String
    rsh = build "rsh" (T [u, T [v, w]], T [T [u, v], w])
\end{code}

Once again, even though |u|, |v| and |w| are implicit arguments, they are resolved at compile time because when using |fork| in a program the result type is known, which means |u| is known, and hence there is no ambiguity and everything can be solved. |fork2| now goes back to how it ought to:

\begin{code}
    fork2 : {u, v : Shp} -> TRuby (T [u, u], T [T [u, v], v]) String
    fork2 = inv pi_1 <:> (inv fork <|> fork) <:> rsh
\end{code}

However, we still have to use this extra datatype |Typed|. If only we could index the syntax trees themselves with their types...


\subsubsection{Indexed functors and monads}

An indexed functor is a functor over the category $a \to Type$ where $a \in Type$. Ideally, a functor $F$ would be specified as being over some category $\mathcal{C}$, which means both indexed and non-indexed functors are represented and implemented in exactly the same way; unfortunately, no programming language to date has this feature. Therefore we will have to make do by defining a new typeclass to represent indexed functors:

\begin{code}
interface IFunctor (h : (a -> Type) -> a -> Type) where
    imap : {f, g : a -> Type} -> (forall x . f x -> g x) -> (forall x . h f x -> h g x)
\end{code}

An indexed functor |h| takes an indexer |k : a -> Type| and an index |t : a|. The type of leaves is not |a| but rather |k p| for values |p : a|. This means each leaf (and indeed each subtree) can be of a different type, which is dictated by the indexer |k|. The indexed map, |imap|, can't just take a function |a -> b| because |a| and |b| are unknown as they are determined by the indexer. Therefore it takes a function |f x -> g x| that works on \emph{all} |x|, and lifts it to act on |h f|.

For now, we will be setting |k = Const t| for some type |t|. This means that all leaves and subtrees will be of the same type, which is no different than the usual non-indexed version. The crucial difference is that we will still be indexed, and those indices will exist at the type level and at compile-time, rather than at runtime.

Here are our Ruby combinators expressed as an indexed functor:

\iffalse
\begin{code}
namespace Ind
\end{code}
\fi

\begin{code}
    data RComb : (k : (Shp, Shp) -> Type) -> (Shp, Shp) -> Type where
        Seq : k (a, b) -> k (b, c) -> RComb k (a, c)
        Par : k (a, b) -> k (c, d) -> RComb k (T [a, c], T [b, d])
        Inv : k (a, b) -> RComb k (b, a)
    
    IFunctor RComb where
        imap f (Seq q r) = Seq (f q) (f r)
        imap f (Par q r) = Par (f q) (f r)
        imap f (Inv q) = Inv (f q)
\end{code}

We have bypassed the |Typed| datatype completely by putting the types directly into the effect's signature. Of course we now need to make an indexed version of |Monad| as well:

\begin{code}
interface IFunctor m => IMonad (a : Type) (m : (a -> Type) -> a -> Type) where
    skip : {f : a -> Type} -> (forall x . f x -> m f x)
    extend : {f, g : a -> Type} -> (forall x . f x -> m g x) -> (forall x . m f x -> m g x)

(>>=) : {a : Type} -> {m : (a -> Type) -> a -> Type} -> IMonad a m => {f, g : a -> Type}
     -> (forall x . m f x -> (forall y . f y -> m g y) -> m g x)
(v >>= k) = extend {f} k v

pure : {a : Type} -> {m : (a -> Type) -> a -> Type} -> IMonad a m => {f : a -> Type}
    -> (forall x . f x -> m f x)
pure = skip {f}
\end{code}

We also overload the |>>=| and |pure| operators to work with |IMonad| rather than |Monad|. Our implementation of |IFree| is straightforward:

\begin{code}
data IFree : (f : (a -> Type) -> a -> Type) -> (c : a -> Type) -> a -> Type where
    Ret : forall x . c x -> IFree f c x
    Do : forall x . f (IFree f c) x -> IFree f c x

{a : Type} -> {h : (a -> Type) -> a -> Type} -> IFunctor h => IFunctor (IFree h) where
    imap k (Ret x) = Ret (k x)
    imap k (Do op) = Do (imap {f=IFree h f} (imap {f} k) op)

{a : Type} -> {h : (a -> Type) -> a -> Type} -> IFunctor h => IMonad a (IFree h) where
    skip x = Ret x
    extend k (Ret x) = k x
    extend k (Do op) = Do (imap {f=IFree h f} (extend {m=IFree h} k) op)
\end{code}

Sometimes the typechecker gets a bit lost so we have to help it along by manually passing |f| and |g| to |imap| and |extend|, but since we've done this here it won't be necessary to do it anywhere else. Note that we skip defining the |IApplicative|.\fxwarning{Atkey} Indexed applicatives are a little tricky and don't exist in general, extra restrictions are needed if you want to implement them but since we won't need them for this paper we'll just leave them out.

As mentioned already, for now we won't take advantage of the special feature of indexing that allows the type of the subtrees to vary; rather, we'll be using a constant indexer:

\begin{code}
Const : Type -> (x : Type) -> x -> Type
Const t _ _ = t
\end{code}
\iffalse
\begin{code}
namespace Ind
\end{code}
\fi
\begin{code}
    Ruby : Type -> (Shp, Shp) -> Type
    Ruby a x = IFree RComb (Const a (Shp, Shp)) x
\end{code}

The smart constructors are implented as

\begin{code}
    (<:>) : Ruby t (a, b) -> Ruby t (b, c) -> Ruby t (a, c)
    (q <:> r) = Do (Seq q r)
    
    (<|>) : Ruby t (a, b) -> Ruby t (c, d) -> Ruby t (T [a, c], T [b, d])
    (q <|> r) = Do (Par q r)
    
    inv : Ruby t (a, b) -> Ruby t (b, a)
    inv q = Do (Inv q)
\end{code}

Once again, the code for |fork2| is

\begin{code}
    build : t -> (x : (Shp, Shp)) -> Ruby t x
    build v _ = Ret v

    fork : {u : Shp} -> Ruby String (u, T [u, u])
    fork = build "fork" (u, T [u, u])

    pi_1 : {u, v : Shp} -> Ruby String (T [u, v], u)
    pi_1 = build "pi1" (T [u, v], u)
    
    rsh : {u, v, w : Shp} -> Ruby String (T [u, T [v, w]], T [T [u, v], w])
    rsh = build "rsh" (T [u, T [v, w]], T [T [u, v], w])

    fork2 : {u, v : Shp} -> Ruby String (T [u, u], T [T [u, v], v])
    fork2 = inv pi_1 <:> (inv fork <|> fork) <:> rsh
\end{code}

At this point, we are well on our way to writing type-safe Ruby as a simple embedded language. However, there are additional considerations to take into account.


\subsection{Typed Ruby, by signal direction}

Consider for a moment the actual purpose of Ruby: to describe hardware. Unfortunately (or perhaps fortunately, if you dislike relational programming) hardware is not relational, it is functional. As mentioned, functions are a strict subset of relations. It therefore follows that Ruby as we have implemented it can describe impossible circuits. For example, if $add$ is defined by $\left<x,y\right>\ add\ (x+y)$ then the circuit $add\ ;\ add^{-1}$ expresses the relation \[
    \left<x,y\right> (add\ ;\ add^{-1}) \left<a,b\right> \iff x + y = a + b
.\] However, as a circuit, an adder has data flowing from domain to codomain: $x$ and $y$ are inputs, and $x + y$ is the output. The below picture makes the problem obvious: the wire connecting the left and right adders is being driven twice, which would be completely rejected by a synthesis tool, and even if it were accepted would result in completely undefined behaviour (the relational behaviour would certainly not be preserved). It would therefore be useful if we could index our syntax tree by both the signal structure and signal direction.

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}[>={Stealth[inset=0pt,length=8pt,angle'=40]}]
        \node at (0, 0.65) (x) {$x$};
        \node at (0, 1.35) (y) {$y$};
        \node at (2, 1) (add1) {$add$};
        \node at (4.5, 1.4) (sum) {$x + y = a + b$};
        \node at (7, 1) (add2) {$add^{-1}$};
        \node at (9, 0.65) (a) {$a$};
        \node at (9, 1.35) (b) {$b$};

        \draw[->] (x) -- (1, 0.65);
        \draw[->] (y) -- (1, 1.35);
        \draw (1, 0) rectangle (3, 2);
        \draw[->] (3, 1) -- (4.5, 1);
        \draw[<-] (4.5, 1) -- (6, 1);
        \draw (6, 0) rectangle (8, 2);
        \draw[<-] (8, 0.65) -- (a);
        \draw[<-] (8, 1.35) -- (b);
    \end{tikzpicture}
    \caption{A picture of $add\ ;\ add^{-1}$. Arrows represent the direction of data flow.}
    \label{fig:addfail}
\end{figure}

This new information requires an update to our typing rules. Earlier, we used $A,B,\ldots$ to vary over empty tuples, which denote the structure of the domain and codomain. Now we will vary over tuples of directions. There are two ways to set this up: we can let directions be \emph{Right} and \emph{Left} (i.e. arrow pointing to the right or to the left), or \emph{In} and \emph{Out} (i.e. arrows entering the block or leaving the block). We choose to use \emph{In} and \emph{Out} as the physical direction is not imporant compared to whether the wire is driving the block as an input or it is being driven as an output. We can now adjust our typing rules, specifically Seq: \fxwarning{Binary algebras}

\begin{figure}[htpb]
    \centering
    \begin{subfigure}[c]{0.4\textwidth}
        \begin{prooftree}
            \hypo{Q : a \leftrightarrow b}
            \hypo{R : \overline{b} \leftrightarrow c}
            \infer2[Seq]{(Q;R) : a \leftrightarrow c}
        \end{prooftree}    
    \end{subfigure}
    \quad where \quad
    \begin{subfigure}[c]{0.4\textwidth}
        \begin{align*}
            \overline{In} &= Out \\
            \overline{Out} &= In \\
            \overline{\left<a_{1},\ldots,a_{n}\right>} &= \left<\overline{a_{1}},\ldots,\overline{a_{n}}\right>
        \end{align*}
    \end{subfigure}
    \caption{New typing rule for Seq, and definition of the complement operation.}
    \label{fig:typing2}
\end{figure}

The complement is needed because for sequential composition, outputs of the first block must match with inputs of the second block, and vice-versa. The typing rules for $Par$ and $Inv$ are unchanged.

The types of polymorphic primitives such as $fork$ have changed dramatically. We will need to define two new predicates, $\iota$ and $\omega$. $\iota(a)$ holds iff all elements of $a$ are $In$, and likewise $\omega(a)$ holds iff all elements of $a$ are $Out$. We have the following laws:

\begin{align*}
    \iota(In) \qquad \neg \iota(Out) \qquad \iota(\left<a_{1},\ldots,a_{n}\right>) &\iff \bigwedge \iota(a_{k}) \\
    \omega(Out) \qquad \neg \omega(In) \qquad \omega(\left<a_{1},\ldots,a_{n}\right>) &\iff \bigwedge \omega(a_{k}) \\
    \iota(a) \iff \omega(\overline{a})
\end{align*}

For example, the typing rules for our three primitives $\pi_{1}$, $fork$ and $rsh$ are

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{\iota(b)}
        \infer1{\pi_1 : \left<a,b\right> \leftrightarrow \overline{a}}
    \end{prooftree}
    \qquad \qquad
    \begin{prooftree}
        \hypo{\iota(b)}
        \rewrite{}
        \infer1{rsh : \left<a,\left<b,c\right>\right> \leftrightarrow \left<\left<\overline{a},\overline{b}\right>,\overline{c}\right>}
    \end{prooftree}
    \bigbreak
    \begin{prooftree}
        \hypo{\iota(a)}
        \infer1{fork : a \leftrightarrow \left<\overline{a},\overline{a}\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\iota(a)}
        \infer1{fork : \overline{a} \leftrightarrow \left<\overline{a},a\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\iota(a)}
        \infer1{fork : \overline{a} \leftrightarrow \left<a,\overline{a}\right>}
    \end{prooftree}
    \caption{Typing rules for $\pi_{1}$, $fork$ and $rsh$.}
    \label{fig:typingPrims}
\end{figure}

$\pi_{1}$ and $rsh$ are simple enough, but with $fork$ things start to get tricky - there are three different ways of typing a fork, because while the block is completely polymorphic in the shape of its signals, one must be an input and the other two must be outputs.

In practice, however, we would like the type to be free from functions; it may be difficult for type inference to infer information from a signature with complements, so it would be better to express complements as an explicit property $C$ where $C(a,b) \iff a = \overline{b}$. Inferring a proof for $C(a,I)$ for a type $T [a, I]$ is easier for the Idris type system than unifying $\left<a,\overline{a}\right>$ and $T [a, I]$ for instance. Our actual implemetation will therefore use the following typing rules:

\begin{figure}[htpb]
    \centering
    \begin{prooftree}
        \hypo{\iota(b)}
        \hypo{C(a,c)}
        \infer2{\pi_1 : \left<a,b\right> \leftrightarrow c}
    \end{prooftree}
    \qquad \qquad
    \begin{prooftree}
        \hypo{C(a,x)}
        \hypo{C(b,y)}
        \hypo{C(c,z)}
        \infer3{rsh : \left<a,\left<b,c\right>\right> \leftrightarrow \left<\left<x,y\right>,z\right>}
    \end{prooftree}
    \bigbreak
    \begin{prooftree}
        \hypo{\iota(a)}
        \hypo{C(a,b)}
        \infer2{fork : a \leftrightarrow \left<b,b\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\iota(a)}
        \hypo{C(a,b)}
        \infer2{fork : b \leftrightarrow \left<a,b\right>}
    \end{prooftree}
    \quad
    \begin{prooftree}
        \hypo{\iota(a)}
        \hypo{C(a,b)}
        \infer2{fork : b \leftrightarrow \left<b,a\right>}
    \end{prooftree}
    \caption{Better typing rules for $\pi_{1}$, $fork$ and $rsh$.}
    \label{fig:typingPrims2}
\end{figure}

Now there are no functions on the bottom of any of these rules, only free variables that are constrained by the properties on the top. The implementation for $\iota$ is

\iffalse
\begin{code}
namespace Dirs
\end{code}
\fi
\begin{code}
    data Dir = In | Out

    DShp : Type
    DShp = Rose Dir
\end{code}
\hspace{-0.3cm}\begin{minipage}[b]{0.49\textwidth}
    \begin{code}
    mutual
        data Ins : DShp -> Type where
            TIns : Ins' xs -> Ins (T xs)
            VIns : Ins (V In)
    \end{code}
    \end{minipage}
    \begin{minipage}[b]{0.49\textwidth}
    \begin{code}
        data Ins' : Vect n DShp -> Type where
            TIns' : Ins x -> Ins' xs -> Ins' (x :: xs)
            VIns' : Ins' []
    \end{code}
\end{minipage}

\iffalse
\begin{code}
    mutual
        data Ins : DShp -> Type where
            TIns : Ins' xs -> Ins (T xs)
            VIns : Ins (V In)
        
        data Ins' : Vect n DShp -> Type where
            TIns' : Ins x -> Ins' xs -> Ins' (x :: xs)
            VIns' : Ins' []
\end{code}
\fi

The |mutual| block allows for mutual recursion, because Idris is usually define-before-use. We could have opted to combine both defintions together with a heterogeneous vector like |TIns : {xs : Vect n DShp} -> HVect (map Ins xs) -> Ins (T xs)| but heterogeneous vectors are much harder to reason about than plain vectors, so splitting into two datatypes like this is larger syntactically but significantly simpler to use, and easier for the compiler to automatically provide implementations for. |Outs| is defined analogously for $\omega$. $C$ is defined with the same technique:

\iffalse
\begin{code}
    mutual
        data Outs : DShp -> Type where
            TOuts : Outs' xs -> Outs (T xs)
            VOuts : Outs (V Out)
        
        data Outs' : Vect n DShp -> Type where
            TOuts' : Outs x -> Outs' xs -> Outs' (x :: xs)
            VOuts' : Outs' []
\end{code}
\fi

\begin{code}
    mutual
        data Compl : DShp -> DShp -> Type where
            TCompl : Compl' xs ys -> Compl (T xs) (T ys)
            VComplIO : Compl (V In) (V Out)
            VComplOI : Compl (V Out) (V In)
        
        data Compl' : Vect n DShp -> Vect n DShp -> Type where
            TCompl' : Compl x y -> Compl' xs ys -> Compl' (x :: xs) (y :: ys)
            VCompl' : Compl' [] []
\end{code}

\begin{code}
    data RComb : (k : (DShp, DShp) -> Type) -> (DShp, DShp) -> Type where
        Seq : k (a, b) -> k (b', c) -> Compl b b' => RComb k (a, c)
        Par : k (a, b) -> k (c, d) -> RComb k (T [a, c], T [b, d])
        Inv : k (a, b) -> RComb k (b, a)
    
    IFunctor RComb where
        imap f (Seq q r) = Seq (f q) (f r)
        imap f (Par q r) = Par (f q) (f r)
        imap f (Inv q) = Inv (f q)
    
    Ruby : Type -> (DShp, DShp) -> Type
    Ruby a x = IFree RComb (Const a (DShp, DShp)) x

    (<:>) : Ruby t (a, b) -> Ruby t (b', c) -> Compl b b' => Ruby t (a, c)
    (q <:> r) = Do (Seq q r)
    
    (<|>) : Ruby t (a, b) -> Ruby t (c, d) -> Ruby t (T [a, c], T [b, d])
    (q <|> r) = Do (Par q r)
    
    inv : Ruby t (a, b) -> Ruby t (b, a)
    inv q = Do (Inv q)

    build : t -> {x : (DShp, DShp)} -> Ruby t x
    build v {x} = Ret v

    pi_1 : {u, v, w : DShp} -> Ins v => Compl u w => Ruby String (T [u, v], w)
    pi_1 = build "pi1"

    rsh : {a, b, c, x, y, z : DShp} -> (Compl a x, Compl c y, Compl c z) => Ruby String (T [a, T [b, c]], T [T[x, y], z])
    rsh = build "rsh"

    fork'' : {u, v : DShp} -> Ins u => Compl u v => Ruby String (v, T [v, u])
    fork'' = build "fork"

    I, O : DShp
    I = V In
    O = V Out

    fork2 : Ruby String (T [O, I], T [T [O, O], I])
    fork2 = inv pi_1 <:> (inv fork'' <|> fork'') <:> rsh
\end{code}



\subsection{Typed Ruby, by loops}



\section{Making relations functional}

We now have a DSL that enforces three important conditions when composing relations:

\begin{enumerate}
    \item No unconnected signals: Domain and codomain structure must agree.
    \item No badly driven signals: Inputs and outputs must be complementary.
    \item No feedback loops: Circuits are stable and do not have unbroken feedback loops.
\end{enumerate}

The first condition is a basic one required for the composition of relations. The second and third are the exact conditions required to ensure that the resulting relation can be represented by a function, i.e. is implementable in hardware. We call such a relation a causal relation. \fxwarning{Causal}

\subsection{Causal relations and functional interpretations}

A causal relation is a relation whose domain and codomain can be partitioned into two groups, inputs and outputs, such that the outputs can be determined from the inputs. We formalise this as follows:

Let $Q : A \leftrightarrow B$ be a relation. $Q$ is called \emph{causal} if there exist objects $I_{L}, O_{L}, I_{R}, O_{R}$, isomorphisms \[
    i_{L} : A \cong \left<I_{L},O_{L}\right> \qquad i_{R} : B \cong \left<I_{R},O_{R}\right>
\] and a function \[
    f : \left<I_{L},I_{R}\right> \to \left<O_{L},O_{R}\right>
\] such that
\begin{align*}
    a\ Q\ b \iff &a = i_{L}^{-1} \left<\pi_{1} \circ i_{L} (a), \pi_{1}(x)\right> \\
    &b = i_{R}^{-1} \left<\pi_{1} \circ i_{R} (b), \pi_{2}(x)\right> \\
    \text{ where } &x = f \left<\pi_{1} \circ i_{L}(a), \pi_{1} \circ i_{R}(b)\right>
.\end{align*}

All that is to say, we can rearrange the domain and codomain into ins and outs, apply a function to the ins, and then undo the rearrangement to put the outs back where they should be. We call the triple $(i_{L}, i_{R}, f)$ a \emph{functional interpretation} of $Q : A \leftrightarrow B$. A relation may have many functional interpretations. For example, here are two functional interpretations of the identity relation $id : A \leftrightarrow A$:

\begin{align*}
    I_{L} = O_{R} = A, && I_{L} = O_{R} = 1, \\
    I_{R} = O_{L} = 1, && I_{R} = O_{L} = A, \\
    i_{L}(x) = \left<x,1\right>, && i_{L}(x) = \left<1,x\right>, \\
    i_{R}(x) = \left<1,x\right>, && i_{R}(x) = \left<x,1\right>, \\
    f\left<x,y\right> = \left<x,x\right>. && f\left<x,y\right> = \left<y,y\right>
.\end{align*}

\begin{figure}[htpb]
    \centering
    % https://q.uiver.app/?q=WzAsMTIsWzAsMCwiQSJdLFsyLDAsIlxcbGVmdDxJX0EsT19BXFxyaWdodD4iXSxbMiwyLCJJX0EiXSxbMCwyLCJcXGxlZnQ8SV9BLE9fQVxccmlnaHQ+Il0sWzQsMCwiXFxsZWZ0PElfQixPX0JcXHJpZ2h0PiJdLFs2LDAsIkIiXSxbNCwyLCJJX0IiXSxbNiwyLCJcXGxlZnQ8SV9CLE9fQlxccmlnaHQ+Il0sWzMsMiwiXFxsZWZ0PElfQSxJX0JcXHJpZ2h0PiJdLFszLDQsIlxcbGVmdDxPX0EsT19CXFxyaWdodD4iXSxbMCw0LCJPX0EiXSxbNiw0LCJPX0IiXSxbMCwxLCJpX0EiXSxbMSwyLCJcXHBpXzEiXSxbMywwLCJpX0Feey0xfSJdLFs0LDYsIlxccGlfMSIsMl0sWzUsNCwiaV9CIl0sWzcsNSwiaV9CXnstMX0iLDJdLFsyLDgsIlxcdGltZXMiLDFdLFs2LDgsIlxcdGltZXMiLDFdLFs4LDksImYiXSxbOSwxMCwiXFxwaV8xIl0sWzksMTEsIlxccGlfMiIsMl0sWzEwLDMsIlxcdGltZXMiLDFdLFsyLDMsIlxcdGltZXMiLDFdLFs2LDcsIlxcdGltZXMiLDFdLFsxMSw3LCJcXHRpbWVzIiwxXV0=
\[\begin{tikzcd}
	A && {\left<I_A,O_A\right>} && {\left<I_B,O_B\right>} && B \\
	\\
	{\left<I_A,O_A\right>} && {I_A} & {\left<I_A,I_B\right>} & {I_B} && {\left<I_B,O_B\right>} \\
	\\
	{O_A} &&& {\left<O_A,O_B\right>} &&& {O_B}
	\arrow["{i_A}", from=1-1, to=1-3]
	\arrow["{\pi_1}", from=1-3, to=3-3]
	\arrow["{i_A^{-1}}", from=3-1, to=1-1]
	\arrow["{\pi_1}"', from=1-5, to=3-5]
	\arrow["{i_B}", from=1-7, to=1-5]
	\arrow["{i_B^{-1}}"', from=3-7, to=1-7]
	\arrow["\times"{description}, from=3-3, to=3-4]
	\arrow["\times"{description}, from=3-5, to=3-4]
	\arrow["f", from=3-4, to=5-4]
	\arrow["{\pi_1}", from=5-4, to=5-1]
	\arrow["{\pi_2}"', from=5-4, to=5-7]
	\arrow["\times"{description}, from=5-1, to=3-1]
	\arrow["\times"{description}, from=3-3, to=3-1]
	\arrow["\times"{description}, from=3-5, to=3-7]
	\arrow["\times"{description}, from=5-7, to=3-7]
\end{tikzcd}\]
    \caption{Diagram showing how to rearrange the domain and codomain, apply the functional interpretation and return back to the domain and codomain.}
    \label{fig:iso}
\end{figure}

We can use $i_{L}$ to get the input and output on the domain, and then take the first element of that pair which is the input $I_{L}$. Likewise, we can do the same to $B$ to get $I_{R}$. We then combine them into a pair and apply our functional interpretation, yielding the outputs $O_{L}$ and $O_{R}$. Pair each of those back up with the inputs $I_{L}$ and $I_{R}$ and apply the isomorphism in reverse to get back into tuple form. The result should be that the input values on both sides are unchanged, but the output values have been populated.

To implement this, suppose we have two relations $Q : A \leftrightarrow B$ and $R : B \leftrightarrow C$ that we would like to compose, and suppose further that both of these relations are causal. That means we have the following:

\begin{itemize}
    \item Objects $I_{L},\, O_{L},\, I_{R},\, O_{R},\, I_{L}',\, O_{L}',\, I_{R}',\, O_{R}'$.
    \item Isomorphisms $i_{L} : A \cong \left<I_{L},O_{L}\right>$, $i_{R} : B \cong \left<I_{R},O_{R}\right>$, $i_{L}' : B \cong \left<I_{L}',O_{L}'\right>$, $i_{R}' : C \cong \left<I_{R}',O_{R}'\right>$.
    \item Functions $f : \left<I_{L},I_{R}\right> \to \left<O_{L},O_{R}\right>$ and $g : \left<I_{L}',I_{R}'\right> \to \left<O_{L}',O_{R}'\right>$.
\end{itemize}

Furthermore, for $Q$ and $R$ to compose we must have that the inputs and outputs on the codomain of $Q$ are complement to the inputs and outputs on the domain of $R$; that is to say, \[
    I_{R} = O_{L}',\, O_{R} = I_{L}',\, i_{R} = swap \circ i_{L}'
\] where $swap \left<x,y\right> = \left<y,x\right>$.

Cleaning up a bit, the composition looks like the diagram below, where \[
    a \in I_{L},\, b \in O_{L},\, c = w \in O_{R},\, d = x \in I_{R},\, y \in O_{R}',\, z \in I_{R}'
.\] Furthermore, $\left<b,c\right> = f \left<a,x\right>$ and $\left<x,y\right> = g \left<c,z\right>$. These are mutually recursive definitions for $b$ and $y$, the outputs of $(Q\,;R)$, in terms of $a$ and $z$, the inputs of $(Q\,;R)$. It follows then that if $Q$ and $R$ are causal, so is $(Q\,;R)$, with the isomorphisms for $A$ and $C$ being the same as those for $Q$ and $R$, and the interpretation function being
\begin{align*}
    h : \left<I_{L},I_{R}'\right> &\to \left<O_{L},O_{R}'\right>, \\
    h \left<a,z\right> &= \left<b,y\right> \text{ where } \\
    \left<b,c\right> &= f \left<a,x\right>, \\
    \left<x,y\right> &= g \left<c,z\right>
.\end{align*}

\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}[>={Stealth[inset=0pt,length=8pt,angle'=40]}]
        \node at (0, 1) (a) {$a$};
        \node at (0, 2) (b) {$b$};
        \node at (4.5, 0.7) (c) {$c$};
        \node at (4.5, 2.3) (d) {$d$};
        \node at (6.5, 0.7) (w) {$w$};
        \node at (6.5, 2.3) (x) {$x$};
        \node at (11, 1) (y) {$y$};
        \node at (11, 2) (z) {$z$};

        \node at (2.5, 1.5) (q) {$Q$};
        \node at (8.5, 1.5) (r) {$R$};

        \draw (1, 0) rectangle (4, 3);
        \draw (7, 0) rectangle (10, 3);

        \draw[->] (a) -- (1, 1);
        \draw[<-] (b) -- (1, 2);
        \draw[->] (4, 1) -- (7, 1);
        \draw[<-] (4, 2) -- (7, 2);
        \draw[->] (10, 1) -- (y);
        \draw[<-] (10, 2) -- (z);
    \end{tikzpicture}
    \caption{A picture of $(Q\,;R)$.}
    \label{fig:functionalSeq}
\end{figure}

If there is no delay on the feedback path from $c$ back to itself (or equivalently, from $x$ back to itself) then this is likely to spiral off into infinite recursion, and even if it doesn't it's not a valid hardware implementation. However, if we can guarantee that there are delays then the mutual recursion is guaranteed to have a base case and the resulting circuit is well-defined.

%%
%% Acknowledgements
\begin{acks}
    \fxwarning{Acknowledgements}
\end{acks}

%%
%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}

\listoffixmes

\end{document}
\endinput
